{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) [+-*/] (3, 4) = (3, 4)\n",
      "(3, 1) [+-*/] (1, 4) = (3, 4)\n",
      "(3, 1) [+-*/] (3, 1) = (3, 1)\n",
      "(3, 4) [+-*/] (3, 1) = (3, 4)\n",
      "(3, 4) [+-*/] (1, 4) = (3, 4)\n",
      "(3, 4) [+-*/] (3, 4) = (3, 4)\n"
     ]
    }
   ],
   "source": [
    "# different broadcast rules in numpy\n",
    "shapes = [[(3,1),(3,4)],[(3,1),(1,4)],[(3,1),(3,1)],[(3,4),(3,1)],[(3,4),(1,4)],[(3,4),(3,4)]]\n",
    "for aa,bb in shapes:\n",
    "    a = np.random.randn(*aa)\n",
    "    b = np.random.randn(*bb)\n",
    "    print(a.shape,'[+-*/]',b.shape,'=',(a-b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (100, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = np.random.rand(100).reshape(1,-1)\n",
    "w2 = np.random.rand(100).reshape(-1,1)\n",
    "np.dot(w1,w2)\n",
    "w1.shape,w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple logistic regression from scratch and use gradient descent\n",
    "\n",
    "class LR:\n",
    "    def __init__(self,inp_size=2) -> None:\n",
    "        # z = x1 * w1 + x2 * w2 + b\n",
    "        self.w = np.random.rand(inp_size,1) #  left features X right features[<>]\n",
    "        self.b = np.random.rand(1,1)\n",
    "    \n",
    "    def fpass(self,x):\n",
    "        z = x @ self.w + self.b\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def BCE(self,target,predicted):\n",
    "        return -(target*np.log(predicted) + (1-target)*np.log(1-predicted))\n",
    "    \n",
    "    def train(self,x,y,epoch=100,lr=0.01):\n",
    "        batch = x.shape[0]\n",
    "        for _ in range(1,epoch+1):\n",
    "            if _%10==0:\n",
    "                print(self.w,self.b)\n",
    "            z = x @ self.w + self.b # (z = xw + b)\n",
    "            a = self.sigmoid(z)\n",
    "            loss = self.BCE(y,a)\n",
    "         \n",
    "            da = -(y/a - (1-y)/(1-a))\n",
    "            dz = da * (a - a**2) #dz = a - y\n",
    "            dw = (x.T @ dz)/batch\n",
    "            db = dz.sum(axis=0,keepdims=True)/batch\n",
    "\n",
    "            self.w -= lr*dw\n",
    "            self.b -= lr*db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36002527]\n",
      " [0.93590538]] [[0.70417022]]\n",
      "[[0.34425671]\n",
      " [0.91976212]] [[0.67396657]]\n",
      "[[0.32886769]\n",
      " [0.90394858]] [[0.64451325]]\n",
      "[[0.31385917]\n",
      " [0.88846648]] [[0.6158102]]\n",
      "[[0.29923137]\n",
      " [0.87331686]] [[0.58785601]]\n",
      "[[0.28498377]\n",
      " [0.85850015]] [[0.56064798]]\n",
      "[[0.27111518]\n",
      " [0.84401614]] [[0.53418209]]\n",
      "[[0.25762372]\n",
      " [0.82986402]] [[0.50845315]]\n",
      "[[0.24450688]\n",
      " [0.81604238]] [[0.4834548]]\n",
      "[[0.23176154]\n",
      " [0.80254927]] [[0.4591796]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = LR(2)\n",
    "x = np.random.rand(64,2)\n",
    "y = np.random.rand(64,1)\n",
    "model.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple logistic regression from scratch and use gradient descent\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,layer_sizes=[]) -> None:\n",
    "        # z = x1 * w1 + x2 * w2 + b\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.layers = len(layer_sizes) - 1\n",
    "        self.w = [np.random.randn(layer_sizes[i],layer_sizes[i+1])*0.001 for i in range(len(layer_sizes)-1)] #  left features X right features[row --> all weights of a single feature, col -->all weights of a single neuron]\n",
    "        self.b = [np.random.randn(1,layer_sizes[i+1])*0 for i in range(len(layer_sizes)-1)]\n",
    "\n",
    "    \n",
    "    def fpass(self,x):\n",
    "        a = x\n",
    "        for i in range(self.layers):\n",
    "            z = a @ self.w[i] + self.b[i]\n",
    "            if i == self.layers-1:\n",
    "                if self.layer_sizes[-1] == 1:\n",
    "                    a = self.sigmoid(z)\n",
    "                    return a.round()\n",
    "                else:\n",
    "                    a = self.relu(z)\n",
    "            else:\n",
    "                a=self.relu(z)\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self,z):\n",
    "        return z*(1-z)\n",
    "    \n",
    "    def relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "    \n",
    "    def relu_prime(self,z):\n",
    "        return np.where(z>0,1,0)\n",
    "    \n",
    "    def tanh(self,z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def BCE(self,target,predicted):\n",
    "        return -(target*np.log(predicted) + (1-target)*np.log(1-predicted))\n",
    "    \n",
    "    def BCE_prime(self,target,predicted):\n",
    "        return -(target/predicted - (1-target)/(1-predicted))\n",
    "    \n",
    "    def train(self,x,y,lr=0.001):\n",
    "        batch = x.shape[0]\n",
    "        a = []\n",
    "        z = []\n",
    "        a.append(x)\n",
    "\n",
    "        layers = len(self.layer_sizes)-1\n",
    "\n",
    "        for i in range(layers):\n",
    "            z.append(a[i] @ self.w[i] + self.b[i])\n",
    "            if i == layers-1:\n",
    "                if self.layer_sizes[-1] == 1:\n",
    "                    a.append(self.sigmoid(z[i]))\n",
    "                else:\n",
    "                    a.append(self.relu(z[i]))\n",
    "            else:\n",
    "                a.append(self.relu(z[i]))\n",
    "        loss = self.BCE(y,a[-1])\n",
    "        print(\"loss:\",loss.sum()/batch)\n",
    "            \n",
    "        \n",
    "        \n",
    "        for i in range(layers,0,-1):\n",
    "            if i == layers:\n",
    "                da = self.BCE_prime(y,a[i])/batch\n",
    "                dz = da * self.sigmoid_prime(a[i]) # we give a[i] cuz in self.sig_prime the output is in term of a not z.\n",
    "            else:\n",
    "                da = dz @ self.w[i].T \n",
    "                dz = da * self.relu_prime(z[i-1])\n",
    "            dw = (a[i-1].T @ dz)\n",
    "            db = dz.sum(axis=0,keepdims=True)\n",
    "            \n",
    "            self.w[i-1] -= lr*dw\n",
    "            self.b[i-1] -= lr*db\n",
    "\n",
    "        \"\"\"\n",
    "        # This is a 2 layer coded out fpass and bpass.\n",
    "\n",
    "        z1 = x @ self.w[0] + self.b[0] # (z = xw + b)--> (batch,inp) @ (inp,hidden) = (batch,hidden)\n",
    "        a1 = self.relu(z1) # (batch,hidden)\n",
    "        z2 = a1 @ self.w[1] + self.b[1] # (batch,hidden) @ (hidden,out) = (batch,out)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        loss = self.BCE(y,a2)\n",
    "\n",
    "\n",
    "        da2 = self.BCE_prime(y,a2)/batch\n",
    "        dz2 = da2 * self.sigmoid_prime(a2) #dz = a - y\n",
    "        dw2 = (a1.T @ dz2)\n",
    "        db2 = dz2.sum(axis=0,keepdims=True)\n",
    "\n",
    "        da1 = dz2 @ self.w[1].T # (batch,out(right)) @ (out(right),hidden(left)) = (batch,hidden)\n",
    "        dz1 = da1 * self.relu_prime(z1) #dz = 0 if z<=0 else 1\n",
    "        dw1 = (x.T @ dz1)\n",
    "        db1 = dz1.sum(axis=0,keepdims=True)\n",
    "        \n",
    "\n",
    "        self.w[0] -= lr*dw1\n",
    "        self.b[0] -= lr*db1\n",
    "        self.w[1] -= lr*dw2\n",
    "        self.b[1] -= lr*db2\n",
    "        \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "dataset = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Step 2: Preprocess Data\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6931471587015235\n",
      "loss: 0.6908321408412584\n",
      "loss: 0.6886809231273315\n",
      "loss: 0.6864903750686896\n",
      "loss: 0.6846470902837559\n",
      "loss: 0.6825643047169309\n",
      "loss: 0.6810036874056938\n",
      "loss: 0.6790137450870052\n",
      "loss: 0.6777127076612973\n",
      "loss: 0.6758023176143432\n",
      "loss: 0.6747397767196927\n",
      "loss: 0.6728971259100368\n",
      "loss: 0.6720538334021934\n",
      "loss: 0.6702684450525938\n",
      "loss: 0.6696268330277015\n",
      "loss: 0.6678894316134136\n",
      "loss: 0.6674334675615067\n",
      "loss: 0.6657358635553995\n",
      "loss: 0.6654509127325352\n",
      "loss: 0.6637858880006908\n",
      "loss: 0.6636585892084454\n",
      "loss: 0.6620197946094888\n",
      "loss: 0.6620379431651413\n",
      "loss: 0.6604198083537887\n",
      "loss: 0.6605722541537865\n",
      "loss: 0.6589699033405442\n",
      "loss: 0.6592464522366754\n",
      "loss: 0.6576556282453907\n",
      "loss: 0.6580469565770324\n",
      "loss: 0.6564639505963322\n",
      "loss: 0.6569615266643201\n",
      "loss: 0.6553831212093751\n",
      "loss: 0.65597913120382\n",
      "loss: 0.654402543735519\n",
      "loss: 0.6550898258934653\n",
      "loss: 0.653512658768916\n",
      "loss: 0.6542846424659485\n",
      "loss: 0.6527048404434859\n",
      "loss: 0.6535554984524912\n",
      "loss: 0.6519713045408873\n",
      "loss: 0.6528950967763264\n",
      "loss: 0.651305023991102\n",
      "loss: 0.6522968600090342\n",
      "loss: 0.6506996547186996\n",
      "loss: 0.6517548483645018\n",
      "loss: 0.6501494656489916\n",
      "loss: 0.6512637015659785\n",
      "loss: 0.6496492831464167\n",
      "loss: 0.6508185789428725\n",
      "loss: 0.649194425461367\n",
      "loss: 0.6504151109085924\n",
      "loss: 0.64878066201005\n",
      "loss: 0.6500493498379116\n",
      "loss: 0.6484041680143998\n",
      "loss: 0.6497177264087303\n",
      "loss: 0.648061482793578\n",
      "loss: 0.6494170150974259\n",
      "loss: 0.6477494756184478\n",
      "loss: 0.6491443002613604\n",
      "loss: 0.647465311048309\n",
      "loss: 0.6488969426578283\n",
      "loss: 0.6472064235145947\n",
      "loss: 0.6486725579283056\n",
      "loss: 0.6469704837570983\n",
      "loss: 0.6484689824769262\n",
      "loss: 0.6467553843121706\n",
      "loss: 0.6482842608181627\n",
      "loss: 0.6465592168963649\n",
      "loss: 0.6481166248414121\n",
      "loss: 0.6463802476030853\n",
      "loss: 0.6479644690233378\n",
      "loss: 0.6462169072787638\n",
      "loss: 0.6478263450532784\n",
      "loss: 0.6460677710945869\n",
      "loss: 0.6477009352318022\n",
      "loss: 0.6459315407530154\n",
      "loss: 0.6475870422834251\n",
      "loss: 0.64580703839403\n",
      "loss: 0.6474835816603894\n",
      "loss: 0.6456931974209246\n",
      "loss: 0.6473895779596771\n",
      "loss: 0.6455890475409144\n",
      "loss: 0.6473041344077561\n",
      "loss: 0.6454937031926326\n",
      "loss: 0.6472264400066412\n",
      "loss: 0.6454063648421213\n",
      "loss: 0.6471557621239487\n",
      "loss: 0.6453262964519653\n",
      "loss: 0.6470914259693418\n",
      "loss: 0.6452528272733304\n",
      "loss: 0.6470328226564698\n",
      "loss: 0.645185357735846\n",
      "loss: 0.6469794042779424\n",
      "loss: 0.6451233277913029\n",
      "loss: 0.6469306629014033\n",
      "loss: 0.6450662318964759\n",
      "loss: 0.6468861435089484\n",
      "loss: 0.6450136081490386\n",
      "loss: 0.646845418456378\n",
      "loss: 0.6449650394266297\n",
      "loss: 0.6468081125270175\n",
      "loss: 0.6449201298008025\n",
      "loss: 0.6467738692472982\n",
      "loss: 0.6448785274915945\n",
      "loss: 0.6467423740683333\n",
      "loss: 0.6448399025303464\n",
      "loss: 0.6467133281224132\n",
      "loss: 0.6448039573221027\n",
      "loss: 0.6466864657711844\n",
      "loss: 0.6447704187982946\n",
      "loss: 0.6466615336074366\n",
      "loss: 0.6447390231269569\n",
      "loss: 0.6466383035338822\n",
      "loss: 0.6447095318966733\n",
      "loss: 0.6466165584312624\n",
      "loss: 0.6446817249833289\n",
      "loss: 0.6465961046829344\n",
      "loss: 0.6446554026582035\n",
      "loss: 0.6465767590766095\n",
      "loss: 0.6446303757685768\n",
      "loss: 0.6465583565460358\n",
      "loss: 0.6446064643420919\n",
      "loss: 0.6465407237309936\n",
      "loss: 0.6445834851481509\n",
      "loss: 0.6465237082965593\n",
      "loss: 0.6445612858651745\n",
      "loss: 0.6465071777493677\n",
      "loss: 0.6445397212235023\n",
      "loss: 0.6464909984294972\n",
      "loss: 0.64451863006908\n",
      "loss: 0.6464750246427157\n",
      "loss: 0.6444978745776803\n",
      "loss: 0.6464591426879925\n",
      "loss: 0.6444773203710122\n",
      "loss: 0.6464432166667702\n",
      "loss: 0.6444568288002033\n",
      "loss: 0.6464271356408727\n",
      "loss: 0.6444362694741484\n",
      "loss: 0.6464107768222291\n",
      "loss: 0.6444155157658983\n",
      "loss: 0.6463940223994389\n",
      "loss: 0.644394437357752\n",
      "loss: 0.6463767532370781\n",
      "loss: 0.6443728975341426\n",
      "loss: 0.6463588510024355\n",
      "loss: 0.64435077066397\n",
      "loss: 0.6463401899795517\n",
      "loss: 0.6443279143618966\n",
      "loss: 0.6463206420853996\n",
      "loss: 0.6443041902652578\n",
      "loss: 0.646300086775058\n",
      "loss: 0.6442794554573918\n",
      "loss: 0.6462783815983881\n",
      "loss: 0.6442535801172827\n",
      "loss: 0.6462553828209426\n",
      "loss: 0.6442264131606156\n",
      "loss: 0.6462309549214571\n",
      "loss: 0.6441977793182223\n",
      "loss: 0.6462049433634381\n",
      "loss: 0.6441675115964112\n",
      "loss: 0.6461772006111426\n",
      "loss: 0.6441354432989902\n",
      "loss: 0.6461475262515062\n",
      "loss: 0.6441013827373351\n",
      "loss: 0.6461157698201415\n",
      "loss: 0.6440651276264896\n",
      "loss: 0.6460817317754951\n",
      "loss: 0.6440264796051387\n",
      "loss: 0.6460452005611761\n",
      "loss: 0.6439852179069675\n",
      "loss: 0.6460059722748691\n",
      "loss: 0.643941089195315\n",
      "loss: 0.645963789299596\n",
      "loss: 0.6438938477558648\n",
      "loss: 0.6459184370382841\n",
      "loss: 0.6438432173059371\n",
      "loss: 0.6458696354738042\n",
      "loss: 0.6437889352818083\n",
      "loss: 0.645817101725678\n",
      "loss: 0.6437306527651635\n",
      "loss: 0.6457605135107769\n",
      "loss: 0.6436680734063039\n",
      "loss: 0.6456996022209537\n",
      "loss: 0.6436008067927573\n",
      "loss: 0.6456339651459565\n",
      "loss: 0.6435284944879985\n",
      "loss: 0.6455632383849591\n",
      "loss: 0.6434507290361081\n",
      "loss: 0.6454870359332658\n",
      "loss: 0.6433670544267412\n",
      "loss: 0.6454049050605035\n",
      "loss: 0.6432770401903681\n",
      "loss: 0.6453163870239924\n",
      "loss: 0.6431801318376825\n",
      "loss: 0.6452210014094586\n",
      "loss: 0.6430757654706811\n",
      "loss: 0.6451181842751887\n",
      "loss: 0.6429634267109302\n",
      "loss: 0.6450073887875577\n",
      "loss: 0.6428424456913912\n",
      "loss: 0.6448879713532671\n",
      "loss: 0.6427121303934397\n",
      "loss: 0.6447592979995581\n",
      "loss: 0.6425716933617198\n",
      "loss: 0.6446205832467407\n",
      "loss: 0.6424204220901493\n",
      "loss: 0.6444711004441698\n",
      "loss: 0.6422574689838756\n",
      "loss: 0.6443100737060895\n",
      "loss: 0.6420818696113452\n",
      "loss: 0.6441364734294873\n",
      "loss: 0.641892775866116\n",
      "loss: 0.6439494548923014\n",
      "loss: 0.6416890570217222\n",
      "loss: 0.6437479735268163\n",
      "loss: 0.6414696566623213\n",
      "loss: 0.6435309672119459\n",
      "loss: 0.6412334035483493\n",
      "loss: 0.6432972404854181\n",
      "loss: 0.6409790513727535\n",
      "loss: 0.6430455821240193\n",
      "loss: 0.6407052594399324\n",
      "loss: 0.6427746429771906\n",
      "loss: 0.6404105162088971\n",
      "loss: 0.6424830238678009\n",
      "loss: 0.6400932319018202\n",
      "loss: 0.6421692256214846\n",
      "loss: 0.6397517874843396\n",
      "loss: 0.6418315804394343\n",
      "loss: 0.6393843781208144\n",
      "loss: 0.6414683847196211\n",
      "loss: 0.6389891329042122\n",
      "loss: 0.6410778509213795\n",
      "loss: 0.6385640729678769\n",
      "loss: 0.6406580412178268\n",
      "loss: 0.6381071006751857\n",
      "loss: 0.6402069421979169\n",
      "loss: 0.6376159812498026\n",
      "loss: 0.6397223167795355\n",
      "loss: 0.6370882315125768\n",
      "loss: 0.6392019240928317\n",
      "loss: 0.6365212662752058\n",
      "loss: 0.6386433928790596\n",
      "loss: 0.635912612040877\n",
      "loss: 0.638043999794305\n",
      "loss: 0.6352594612265019\n",
      "loss: 0.6374011847634043\n",
      "loss: 0.6345587206615574\n",
      "loss: 0.6367121358071658\n",
      "loss: 0.6338074502794778\n",
      "loss: 0.6359738989964474\n",
      "loss: 0.6330023259320903\n",
      "loss: 0.6351835700658467\n",
      "loss: 0.6321399760292906\n",
      "loss: 0.6343378254077683\n",
      "loss: 0.6312170061944097\n",
      "loss: 0.6334334435133567\n",
      "loss: 0.6302296794215216\n",
      "loss: 0.6324671345944421\n",
      "loss: 0.6291744466661567\n",
      "loss: 0.6314353453679806\n",
      "loss: 0.6280473707466879\n",
      "loss: 0.6303346458871912\n",
      "loss: 0.626844708288966\n",
      "loss: 0.629161467530669\n",
      "loss: 0.6255622335529976\n",
      "loss: 0.6279124153607388\n",
      "loss: 0.6241963225115313\n",
      "loss: 0.626583636408161\n",
      "loss: 0.6227426420612215\n",
      "loss: 0.6251720388901892\n",
      "loss: 0.6211974824239265\n",
      "loss: 0.6236738595231608\n",
      "loss: 0.6195569361931349\n",
      "loss: 0.6220861006210309\n",
      "loss: 0.6178174760539011\n",
      "loss: 0.6204053167926185\n",
      "loss: 0.6159754256085164\n",
      "loss: 0.6186289452188969\n",
      "loss: 0.6140273663264262\n",
      "loss: 0.6167545307253537\n",
      "loss: 0.6119709861822427\n",
      "loss: 0.6147791606713695\n",
      "loss: 0.6098032068618453\n",
      "loss: 0.6127018566540043\n",
      "loss: 0.6075218622433345\n",
      "loss: 0.6105211070573986\n",
      "loss: 0.6051255427802914\n",
      "loss: 0.6082365352611355\n",
      "loss: 0.6026132706758884\n",
      "loss: 0.6058476308922957\n",
      "loss: 0.5999854446427468\n",
      "loss: 0.6033552862495952\n",
      "loss: 0.5972425752181543\n",
      "loss: 0.6007612308773094\n",
      "loss: 0.5943858862773614\n",
      "loss: 0.5980695729938018\n",
      "loss: 0.5914162709417142\n",
      "loss: 0.5952805111434918\n",
      "loss: 0.5883400654776058\n",
      "loss: 0.5924020766921928\n",
      "loss: 0.5851585858066234\n",
      "loss: 0.5894379545492978\n",
      "loss: 0.5818794920955309\n",
      "loss: 0.5863940852453916\n",
      "loss: 0.5785092247758851\n",
      "loss: 0.5832779462734702\n",
      "loss: 0.5750557541881942\n",
      "loss: 0.5800969442658576\n",
      "loss: 0.5715277412555482\n",
      "loss: 0.5768610081496214\n",
      "loss: 0.5679341710933008\n",
      "loss: 0.5735809134309744\n",
      "loss: 0.5642866265256011\n",
      "loss: 0.5702672297322022\n",
      "loss: 0.5605954717954716\n",
      "loss: 0.5669306034209709\n",
      "loss: 0.556873985975518\n",
      "loss: 0.5635819591250631\n",
      "loss: 0.5531363627856891\n",
      "loss: 0.5602310221440302\n",
      "loss: 0.5493907712312194\n",
      "loss: 0.5568911549755947\n",
      "loss: 0.5456514868376197\n",
      "loss: 0.5535714574097205\n",
      "loss: 0.5419264606964415\n",
      "loss: 0.5502758269087422\n",
      "loss: 0.5382164421160496\n",
      "loss: 0.5470163277937177\n",
      "loss: 0.5345385287030417\n",
      "loss: 0.5438098695772702\n",
      "loss: 0.5309093719702771\n",
      "loss: 0.5406667895688452\n",
      "loss: 0.5273383254151566\n",
      "loss: 0.5375939079577551\n",
      "loss: 0.5238349709685873\n",
      "loss: 0.5345998037253248\n",
      "loss: 0.520406212130499\n",
      "loss: 0.5316849552939182\n",
      "loss: 0.5170561584099875\n",
      "loss: 0.5288609212448498\n",
      "loss: 0.5137959907298095\n",
      "loss: 0.526131038013649\n",
      "loss: 0.5106289921743004\n",
      "loss: 0.5234983404129468\n",
      "loss: 0.5075613694368569\n",
      "loss: 0.520965522760876\n",
      "loss: 0.504595062705528\n",
      "loss: 0.5185350672448624\n",
      "loss: 0.5017302566092047\n",
      "loss: 0.5162084327286515\n",
      "loss: 0.49897392676681285\n",
      "loss: 0.5139852753738909\n",
      "loss: 0.49632516748553795\n",
      "loss: 0.5118685409565594\n",
      "loss: 0.49378361382007524\n",
      "loss: 0.5098537326260681\n",
      "loss: 0.49134817239451145\n",
      "loss: 0.5079262197250662\n",
      "loss: 0.489011295039336\n",
      "loss: 0.5060689133993952\n",
      "loss: 0.48677215812748525\n",
      "loss: 0.5043015426998422\n",
      "loss: 0.484633419347493\n",
      "loss: 0.5026221321887897\n",
      "loss: 0.4826067357052629\n",
      "loss: 0.5010162923526184\n",
      "loss: 0.48066566403232236\n",
      "loss: 0.49947600174792395\n",
      "loss: 0.4788195856104373\n",
      "loss: 0.4980122658399964\n",
      "loss: 0.47705860067666905\n",
      "loss: 0.4966213411895138\n",
      "loss: 0.4753788084255575\n",
      "loss: 0.49529675892475533\n",
      "loss: 0.4737784398714272\n",
      "loss: 0.494034634362208\n",
      "loss: 0.4722503042205439\n",
      "loss: 0.4928312555824754\n",
      "loss: 0.47079211088465256\n",
      "loss: 0.4916593892954976\n",
      "loss: 0.46938142987723364\n",
      "loss: 0.4904938816916269\n",
      "loss: 0.46803472903226\n",
      "loss: 0.48938792000310183\n",
      "loss: 0.4667559155764716\n",
      "loss: 0.4883361893491645\n",
      "loss: 0.4655404536785092\n",
      "loss: 0.48733223272599646\n",
      "loss: 0.46437737546610747\n",
      "loss: 0.4863784282969957\n",
      "loss: 0.46326609531346824\n",
      "loss: 0.4854834227890627\n",
      "loss: 0.4621748028577193\n",
      "loss: 0.48462972262059767\n",
      "loss: 0.46113618806624823\n",
      "loss: 0.4838006562799463\n",
      "loss: 0.4601402230772535\n",
      "loss: 0.48302007465012387\n",
      "loss: 0.45919020189932996\n",
      "loss: 0.48227292769762703\n",
      "loss: 0.4582892144853801\n",
      "loss: 0.48155210756680317\n",
      "loss: 0.45742871353365383\n",
      "loss: 0.48085024548496114\n",
      "loss: 0.45660522968921297\n",
      "loss: 0.4801858556126587\n",
      "loss: 0.4558119474138344\n",
      "loss: 0.4795539504580979\n",
      "loss: 0.45505313957702187\n",
      "loss: 0.47894859003474544\n",
      "loss: 0.45432392637014457\n",
      "loss: 0.4783676691882991\n",
      "loss: 0.45362622110693024\n",
      "loss: 0.47780411589133026\n",
      "loss: 0.45295425948164997\n",
      "loss: 0.4772437855662703\n",
      "loss: 0.4523072293959679\n",
      "loss: 0.4766998207302157\n",
      "loss: 0.45168227199970223\n",
      "loss: 0.47617186541537576\n",
      "loss: 0.45108150259530955\n",
      "loss: 0.47565735000317133\n",
      "loss: 0.4505132227691901\n",
      "loss: 0.47515246956920426\n",
      "loss: 0.44997812705466983\n",
      "loss: 0.47466996069629325\n",
      "loss: 0.4494782187326043\n",
      "loss: 0.474212719424002\n",
      "loss: 0.44900021872242796\n",
      "loss: 0.47374841502210946\n",
      "loss: 0.4485442021930454\n",
      "loss: 0.47330000540733796\n",
      "loss: 0.4481048048643011\n",
      "loss: 0.47286865643155157\n",
      "loss: 0.4476817832585628\n",
      "loss: 0.4724520973644077\n",
      "loss: 0.4472743275733049\n",
      "loss: 0.4720505213490034\n",
      "loss: 0.44687967945772045\n",
      "loss: 0.4716642400152731\n",
      "loss: 0.446499526064094\n",
      "loss: 0.47129050746184264\n",
      "loss: 0.44613298777797383\n",
      "loss: 0.47092889742261224\n",
      "loss: 0.4457796888060729\n",
      "loss: 0.47057535839389675\n",
      "loss: 0.44544773464447274\n",
      "loss: 0.4702193287111166\n",
      "loss: 0.4451278452684246\n",
      "loss: 0.469873978593151\n",
      "loss: 0.44482389605838524\n",
      "loss: 0.4695452122298962\n",
      "loss: 0.4445307665256488\n",
      "loss: 0.4692243434952469\n",
      "loss: 0.44425295650166113\n",
      "loss: 0.46889518280336057\n",
      "loss: 0.4439871155107094\n",
      "loss: 0.4685770484473763\n",
      "loss: 0.4437239080826368\n",
      "loss: 0.46829085346990545\n",
      "loss: 0.44347066440800575\n",
      "loss: 0.46802027754792724\n",
      "loss: 0.44323165894544775\n",
      "loss: 0.46776813493213165\n",
      "loss: 0.44300281969389516\n",
      "loss: 0.4675257324945196\n",
      "loss: 0.4427830401176923\n",
      "loss: 0.46729043321968666\n",
      "loss: 0.4425716540872494\n",
      "loss: 0.46705856634295584\n",
      "loss: 0.4423588323811616\n",
      "loss: 0.4668454684527409\n",
      "loss: 0.44215871136157064\n",
      "loss: 0.4666415901001222\n",
      "loss: 0.441966752745283\n",
      "loss: 0.46644267810036305\n",
      "loss: 0.4417839642252623\n",
      "loss: 0.46625114255155925\n",
      "loss: 0.44161047507361006\n",
      "loss: 0.46605957938598336\n",
      "loss: 0.4414472893929179\n",
      "loss: 0.46587362933252874\n",
      "loss: 0.44129126384724143\n",
      "loss: 0.4656925930221991\n",
      "loss: 0.4411420068941505\n",
      "loss: 0.465521403881597\n",
      "loss: 0.4410042031953899\n",
      "loss: 0.46535773283879245\n",
      "loss: 0.44087574033866667\n",
      "loss: 0.46520173483462335\n",
      "loss: 0.4407524524527464\n",
      "loss: 0.46504495345510105\n",
      "loss: 0.4406567637894833\n",
      "loss: 0.46484026135096634\n",
      "loss: 0.4405670527868483\n",
      "loss: 0.4646218452875781\n",
      "loss: 0.4404812051989995\n",
      "loss: 0.46440229302651426\n",
      "loss: 0.44039396760334376\n",
      "loss: 0.46417766542506955\n",
      "loss: 0.4403049577395083\n",
      "loss: 0.46396068936225204\n",
      "loss: 0.44021425612731685\n",
      "loss: 0.46375094363830843\n",
      "loss: 0.44012906237145877\n",
      "loss: 0.46353606619122756\n",
      "loss: 0.4400470902823609\n",
      "loss: 0.46332449617615684\n",
      "loss: 0.4399662654023918\n",
      "loss: 0.46311895830816163\n",
      "loss: 0.43988718544560634\n",
      "loss: 0.4629136177379977\n",
      "loss: 0.43981180427108346\n",
      "loss: 0.46270284782702126\n",
      "loss: 0.43973641311778505\n",
      "loss: 0.4625032023144387\n",
      "loss: 0.4396724539435727\n",
      "loss: 0.4623027751121519\n",
      "loss: 0.43960553501690924\n",
      "loss: 0.4621147603253838\n",
      "loss: 0.43954599520425447\n",
      "loss: 0.4619384281477715\n",
      "loss: 0.4394885575029713\n",
      "loss: 0.4617752737943486\n",
      "loss: 0.4394347694912789\n",
      "loss: 0.4616147602865563\n",
      "loss: 0.4393794514036814\n",
      "loss: 0.46146015838527815\n",
      "loss: 0.43932419331091815\n",
      "loss: 0.46131106393237764\n",
      "loss: 0.43927807221141646\n",
      "loss: 0.46115834391718163\n",
      "loss: 0.43922596685577836\n",
      "loss: 0.46101428047943677\n",
      "loss: 0.4391800458351188\n",
      "loss: 0.46086974902674777\n",
      "loss: 0.43913475770042826\n",
      "loss: 0.460731341516651\n",
      "loss: 0.4390909470236256\n",
      "loss: 0.4605947447080616\n",
      "loss: 0.4390529089436671\n",
      "loss: 0.46044314008005804\n",
      "loss: 0.439019418754217\n",
      "loss: 0.46030427516276184\n",
      "loss: 0.4389746803255388\n",
      "loss: 0.460172705388324\n",
      "loss: 0.43893226513695727\n",
      "loss: 0.46004133943658965\n",
      "loss: 0.43889388256489914\n",
      "loss: 0.4599097440717613\n",
      "loss: 0.43885884851366597\n",
      "loss: 0.4597736119162967\n",
      "loss: 0.43882446256621477\n",
      "loss: 0.4596297377745109\n",
      "loss: 0.4388125986096321\n",
      "loss: 0.45947523192969697\n",
      "loss: 0.43879007227531314\n",
      "loss: 0.45932562375886393\n",
      "loss: 0.43876494712801284\n",
      "loss: 0.4591743019461513\n",
      "loss: 0.43874045921960364\n",
      "loss: 0.4590236994061634\n",
      "loss: 0.43871557233193825\n",
      "loss: 0.4588761989752814\n",
      "loss: 0.4386928509370121\n",
      "loss: 0.4587316405230548\n",
      "loss: 0.43866736663944045\n",
      "loss: 0.458581751605843\n",
      "loss: 0.4386556073830775\n",
      "loss: 0.4584467373218686\n",
      "loss: 0.4386288517855717\n",
      "loss: 0.45831201800158483\n",
      "loss: 0.43863029670088843\n",
      "loss: 0.4581376485865264\n",
      "loss: 0.43862153910358037\n",
      "loss: 0.45796531260479956\n",
      "loss: 0.4386317770818414\n",
      "loss: 0.4577398672351237\n",
      "loss: 0.4386514857206496\n",
      "loss: 0.4575250324270959\n",
      "loss: 0.43867196731356545\n",
      "loss: 0.45731514590084904\n",
      "loss: 0.43868197340463994\n",
      "loss: 0.45711459603161375\n",
      "loss: 0.4386880832640694\n",
      "loss: 0.4569041272890355\n",
      "loss: 0.43870507020191957\n",
      "loss: 0.45671406908987694\n",
      "loss: 0.4387071612985939\n",
      "loss: 0.45651624654829076\n",
      "loss: 0.4387171090134184\n",
      "loss: 0.45634598356881717\n",
      "loss: 0.4387078239248958\n",
      "loss: 0.45616993512662346\n",
      "loss: 0.4387105780330388\n",
      "loss: 0.4560081354635132\n",
      "loss: 0.4387025293010863\n",
      "loss: 0.45584692936698384\n",
      "loss: 0.4386983414623957\n",
      "loss: 0.4556962886114225\n",
      "loss: 0.43868445804774336\n",
      "loss: 0.4555452292908565\n",
      "loss: 0.43867293256381945\n",
      "loss: 0.45539818721792535\n",
      "loss: 0.43866454524800236\n",
      "loss: 0.4552633769042202\n",
      "loss: 0.4386491313420153\n",
      "loss: 0.4551121466555524\n",
      "loss: 0.4386578966259617\n",
      "loss: 0.4549842927698431\n",
      "loss: 0.43864944942017353\n",
      "loss: 0.45485456390545553\n",
      "loss: 0.43864663541046983\n",
      "loss: 0.45472247068851496\n",
      "loss: 0.4386490668568051\n",
      "loss: 0.4546059548775434\n",
      "loss: 0.438632559697879\n",
      "loss: 0.45448234402709026\n",
      "loss: 0.438627013842815\n",
      "loss: 0.45435178196355824\n",
      "loss: 0.438631635909461\n",
      "loss: 0.4542404843121815\n",
      "loss: 0.43861825745651495\n",
      "loss: 0.4541254304599787\n",
      "loss: 0.43861105645510334\n",
      "loss: 0.45400349746094176\n",
      "loss: 0.4386103703083737\n",
      "loss: 0.4538849233962808\n",
      "loss: 0.43860486980491237\n",
      "loss: 0.4537621583916113\n",
      "loss: 0.4386015622660388\n",
      "loss: 0.4536737571234061\n",
      "loss: 0.4385857248716697\n",
      "loss: 0.4535806983883968\n",
      "loss: 0.43857965695699\n",
      "loss: 0.4534988863745373\n",
      "loss: 0.4385640737547887\n",
      "loss: 0.4534082790421021\n",
      "loss: 0.4385590662547687\n",
      "loss: 0.45333483516369066\n",
      "loss: 0.4385418467908749\n",
      "loss: 0.4532670467830602\n",
      "loss: 0.438507403958883\n",
      "loss: 0.4532059270337252\n",
      "loss: 0.43846792927481715\n",
      "loss: 0.45314131759052095\n",
      "loss: 0.43843568222895396\n",
      "loss: 0.453081780085608\n",
      "loss: 0.4384001343433943\n",
      "loss: 0.4530157677030172\n",
      "loss: 0.43837062941160776\n",
      "loss: 0.4529656396533882\n",
      "loss: 0.43833472710908616\n",
      "loss: 0.4529030626914272\n",
      "loss: 0.43832108595257824\n",
      "loss: 0.45284241579188456\n",
      "loss: 0.4383021066044001\n",
      "loss: 0.45277713160680627\n",
      "loss: 0.4382860705641941\n",
      "loss: 0.4527213875976099\n",
      "loss: 0.43826355155890173\n",
      "loss: 0.4526551354005278\n",
      "loss: 0.43824989706660356\n",
      "loss: 0.45260702309604356\n",
      "loss: 0.43822396226542343\n",
      "loss: 0.45253901701427723\n",
      "loss: 0.4382137258426542\n",
      "loss: 0.45249004353462025\n",
      "loss: 0.4381909047051052\n",
      "loss: 0.45242419248956783\n",
      "loss: 0.4381772017947129\n",
      "loss: 0.45236622380193875\n",
      "loss: 0.438156972420774\n",
      "loss: 0.4523167553784903\n",
      "loss: 0.4381262837398305\n",
      "loss: 0.45225822621660233\n",
      "loss: 0.43810604389764163\n",
      "loss: 0.45221293736467816\n",
      "loss: 0.4380748641756301\n",
      "loss: 0.4521517024568817\n",
      "loss: 0.4380572777572437\n",
      "loss: 0.4521070915133259\n",
      "loss: 0.4380248880143992\n",
      "loss: 0.45204759134079814\n",
      "loss: 0.4380055644902661\n",
      "loss: 0.4520026271011099\n",
      "loss: 0.4379748935299244\n",
      "loss: 0.4519508053377872\n",
      "loss: 0.43795007395830327\n",
      "loss: 0.45189959376063904\n",
      "loss: 0.437931789073006\n",
      "loss: 0.45186368655073705\n",
      "loss: 0.43789831387126305\n",
      "loss: 0.45180883576467906\n",
      "loss: 0.43788217643797445\n",
      "loss: 0.4517590044366177\n",
      "loss: 0.4378634787828418\n",
      "loss: 0.4517263913583374\n",
      "loss: 0.43782835699334677\n",
      "loss: 0.451678540548293\n",
      "loss: 0.4378074364360761\n",
      "loss: 0.4516317454261428\n",
      "loss: 0.4377847358301468\n",
      "loss: 0.45158497950961346\n",
      "loss: 0.4377659113312395\n",
      "loss: 0.4515570027184137\n",
      "loss: 0.43773238538812076\n",
      "loss: 0.45151626118444593\n",
      "loss: 0.4377114155271764\n",
      "loss: 0.4514820785647312\n",
      "loss: 0.43768916520025264\n",
      "loss: 0.45145313687710437\n",
      "loss: 0.4376612961930246\n",
      "loss: 0.4514315252982884\n",
      "loss: 0.4376244826345916\n",
      "loss: 0.4513947907961095\n",
      "loss: 0.43760210772620045\n",
      "loss: 0.45136184920011757\n",
      "loss: 0.4375760335500571\n",
      "loss: 0.45133241294535015\n",
      "loss: 0.43754760433483075\n",
      "loss: 0.45130046921648426\n",
      "loss: 0.43752251635185546\n",
      "loss: 0.4512714616973218\n",
      "loss: 0.4374959247873731\n",
      "loss: 0.4512352727113129\n",
      "loss: 0.43747654997222535\n",
      "loss: 0.45120604007669407\n",
      "loss: 0.4374524468380568\n",
      "loss: 0.4511900107625988\n",
      "loss: 0.43741655151902487\n",
      "loss: 0.4511606807156547\n",
      "loss: 0.437393473451638\n",
      "loss: 0.4511255588720519\n",
      "loss: 0.4373894937964301\n",
      "loss: 0.45108018367970154\n",
      "loss: 0.4373730842458922\n",
      "loss: 0.4510318780313882\n",
      "loss: 0.43736027377974135\n",
      "loss: 0.45101158520166557\n",
      "loss: 0.4373237036862972\n",
      "loss: 0.45095578534278985\n",
      "loss: 0.4373164213005425\n",
      "loss: 0.4509065562850835\n",
      "loss: 0.4373046399392171\n",
      "loss: 0.4508935213915037\n",
      "loss: 0.4372604571572346\n",
      "loss: 0.45084153799306137\n",
      "loss: 0.4372507605936839\n",
      "loss: 0.45080154451965543\n",
      "loss: 0.43722871589634243\n",
      "loss: 0.4507935934271098\n",
      "loss: 0.43719540191825246\n",
      "loss: 0.45075908726310987\n",
      "loss: 0.4371720738080993\n",
      "loss: 0.4507367987756617\n",
      "loss: 0.43714841964680556\n",
      "loss: 0.4507404196571644\n",
      "loss: 0.4371010154692944\n",
      "loss: 0.4507076303419718\n",
      "loss: 0.4370938446757161\n",
      "loss: 0.450682106427981\n",
      "loss: 0.4370687702589043\n",
      "loss: 0.4506817133688409\n",
      "loss: 0.43702868694705704\n",
      "loss: 0.4506515236727687\n",
      "loss: 0.4370138256628819\n",
      "loss: 0.450632533282423\n",
      "loss: 0.4369978199191875\n",
      "loss: 0.45060516268269096\n",
      "loss: 0.4369758886691261\n",
      "loss: 0.4505920477655088\n",
      "loss: 0.43694619069969126\n",
      "loss: 0.45055883608653025\n",
      "loss: 0.4369385543730162\n",
      "loss: 0.4505650207626574\n",
      "loss: 0.4368960875758913\n",
      "loss: 0.4505376982281495\n",
      "loss: 0.43688157155624385\n",
      "loss: 0.4505047573273093\n",
      "loss: 0.43687286399739667\n",
      "loss: 0.4504920186314194\n",
      "loss: 0.4368480207487243\n",
      "loss: 0.450485819409204\n",
      "loss: 0.4368172460490752\n",
      "loss: 0.4504519437876624\n",
      "loss: 0.4368096417539042\n",
      "loss: 0.450428423302148\n",
      "loss: 0.43679805747246575\n",
      "loss: 0.4504291917754327\n",
      "loss: 0.43675576114884135\n",
      "loss: 0.4503857375240432\n",
      "loss: 0.43674362404188044\n",
      "loss: 0.45036851588264076\n",
      "loss: 0.43671863216668155\n",
      "loss: 0.45036872089889457\n",
      "loss: 0.43667513375510214\n",
      "loss: 0.45032949912148157\n",
      "loss: 0.4366673971527915\n",
      "loss: 0.4503145633077656\n",
      "loss: 0.43664295598820263\n",
      "loss: 0.4503020393070453\n",
      "loss: 0.43660888750808646\n",
      "loss: 0.4502770760824548\n",
      "loss: 0.4365923329355147\n",
      "loss: 0.4502747565190391\n",
      "loss: 0.4365561549808448\n",
      "loss: 0.45023201192481926\n",
      "loss: 0.4365544312719312\n",
      "loss: 0.4502171666080299\n",
      "loss: 0.4365320455099128\n",
      "loss: 0.4502027259313894\n",
      "loss: 0.43650534004051866\n",
      "loss: 0.45017363828646656\n",
      "loss: 0.4364935605354575\n",
      "loss: 0.45016686089667635\n",
      "loss: 0.4364599987126553\n",
      "loss: 0.4501408487168827\n",
      "loss: 0.4364434694216214\n",
      "loss: 0.45012349727018713\n",
      "loss: 0.43641638593711685\n",
      "loss: 0.4501019501145925\n",
      "loss: 0.4363919980895783\n",
      "loss: 0.4500629536468064\n",
      "loss: 0.4363882598439258\n",
      "loss: 0.4500679701256273\n",
      "loss: 0.4363421401464636\n",
      "loss: 0.45003241394016386\n",
      "loss: 0.43633349752043193\n",
      "loss: 0.4500014479881444\n",
      "loss: 0.43631798909423525\n",
      "loss: 0.44999228920239936\n",
      "loss: 0.43628957828465537\n",
      "loss: 0.4499439718895576\n",
      "loss: 0.43628365573076694\n",
      "loss: 0.4499114048888014\n",
      "loss: 0.43626266940754965\n",
      "loss: 0.44987573246939083\n",
      "loss: 0.43625489771981846\n",
      "loss: 0.44985111614807355\n",
      "loss: 0.4362321651882693\n",
      "loss: 0.4498173328211716\n",
      "loss: 0.4362189361379455\n",
      "loss: 0.44980042445450585\n",
      "loss: 0.4361805663678933\n",
      "loss: 0.44980570991292784\n",
      "loss: 0.4361381911736049\n",
      "loss: 0.4497946462889999\n",
      "loss: 0.4361101825520821\n",
      "loss: 0.4497838408069999\n",
      "loss: 0.43609754124125444\n",
      "loss: 0.4497897469171515\n",
      "loss: 0.4360411051029361\n",
      "loss: 0.44977037079972765\n",
      "loss: 0.4360291613168929\n",
      "loss: 0.4497677737721729\n",
      "loss: 0.4359949700560403\n",
      "loss: 0.4497702758549193\n",
      "loss: 0.43597694050038505\n",
      "loss: 0.44975205069625696\n",
      "loss: 0.4359458713265417\n",
      "loss: 0.4497310477750074\n",
      "loss: 0.435937639923158\n",
      "loss: 0.4497197642361516\n",
      "loss: 0.43591452589669244\n",
      "loss: 0.4497237602238025\n",
      "loss: 0.43587825971674743\n",
      "loss: 0.44972067431295343\n",
      "loss: 0.43585407264785586\n",
      "loss: 0.4497119878548421\n",
      "loss: 0.43584647938105886\n",
      "loss: 0.4496768687823806\n",
      "loss: 0.4358242289701696\n",
      "loss: 0.4496625026787678\n",
      "loss: 0.43580139256443984\n",
      "loss: 0.44964148373117\n",
      "loss: 0.43578264262881505\n",
      "loss: 0.4496158682361819\n",
      "loss: 0.4357659743080075\n",
      "loss: 0.4496039310411849\n",
      "loss: 0.43574079797589366\n",
      "loss: 0.4495910287516465\n",
      "loss: 0.4357142248735179\n",
      "loss: 0.44959281069618234\n",
      "loss: 0.43569476646932676\n",
      "loss: 0.4495533131974717\n",
      "loss: 0.4356792593302241\n",
      "loss: 0.4495699602316448\n",
      "loss: 0.43564135882675725\n",
      "loss: 0.4495408887584483\n",
      "loss: 0.43562925144188563\n",
      "loss: 0.4495571538369751\n",
      "loss: 0.4355869038642951\n",
      "loss: 0.44953002495639455\n",
      "loss: 0.4355772817274045\n",
      "loss: 0.4495480673435025\n",
      "loss: 0.4355326490566916\n",
      "loss: 0.44953476170662826\n",
      "loss: 0.4355211726809864\n",
      "loss: 0.44953445130599046\n",
      "loss: 0.4354881883907358\n",
      "loss: 0.4495259368358446\n",
      "loss: 0.4354870975012524\n",
      "loss: 0.4494957849699818\n",
      "loss: 0.43546352955014683\n",
      "loss: 0.44948990915119763\n",
      "loss: 0.43542769967706624\n",
      "loss: 0.4494754444473102\n",
      "loss: 0.43539994036327806\n",
      "loss: 0.4494738234379841\n",
      "loss: 0.43536452244089247\n",
      "loss: 0.44945645745576285\n",
      "loss: 0.43533879089351246\n",
      "loss: 0.44945993640971815\n",
      "loss: 0.4352965068149748\n",
      "loss: 0.4494362236322321\n",
      "loss: 0.4352787317336591\n",
      "loss: 0.44944093562872495\n",
      "loss: 0.4352388708926871\n",
      "loss: 0.44940947239742546\n",
      "loss: 0.4352305261298589\n",
      "loss: 0.4494233200682402\n",
      "loss: 0.4351826704378949\n",
      "loss: 0.44940634162491006\n",
      "loss: 0.43517813731615074\n",
      "loss: 0.4493890782371774\n",
      "loss: 0.43515397844582665\n",
      "loss: 0.44937529844761653\n",
      "loss: 0.43512823724496635\n",
      "loss: 0.44936393442119077\n",
      "loss: 0.435110370859433\n",
      "loss: 0.4493634417461071\n",
      "loss: 0.4350770203461931\n",
      "loss: 0.44935170190120244\n",
      "loss: 0.43505560428805884\n",
      "loss: 0.4493419462925483\n",
      "loss: 0.4350337158759153\n",
      "loss: 0.44933643063863615\n",
      "loss: 0.4350058956800557\n",
      "loss: 0.449325143662701\n",
      "loss: 0.43498595570926485\n",
      "loss: 0.44931442635542623\n",
      "loss: 0.4349671605667298\n",
      "loss: 0.4493261247738277\n",
      "loss: 0.4349229124362519\n",
      "loss: 0.4492971935013536\n",
      "loss: 0.4349194138834534\n",
      "loss: 0.44929999918598645\n",
      "loss: 0.4348849568562975\n",
      "loss: 0.44928934004717436\n",
      "loss: 0.4348663068363183\n",
      "loss: 0.4492800250586549\n",
      "loss: 0.4348434882363366\n",
      "loss: 0.4492682977940497\n",
      "loss: 0.4348263444595321\n",
      "loss: 0.4492679128160919\n",
      "loss: 0.4347991396456729\n",
      "loss: 0.44922868486727335\n",
      "loss: 0.4348039838252979\n",
      "loss: 0.4492041237957959\n",
      "loss: 0.434772293083168\n",
      "loss: 0.4491702093078525\n",
      "loss: 0.434765773771099\n",
      "loss: 0.44917360193465716\n",
      "loss: 0.4347247569128983\n",
      "loss: 0.4491328121101247\n",
      "loss: 0.43472197266061785\n",
      "loss: 0.44912580362434423\n",
      "loss: 0.4346848603795716\n",
      "loss: 0.4490983456967859\n",
      "loss: 0.4346751623787338\n",
      "loss: 0.4491118889877175\n",
      "loss: 0.43463266583688265\n",
      "loss: 0.44908334761366736\n",
      "loss: 0.43462163806524884\n",
      "loss: 0.44908337404153814\n",
      "loss: 0.43458931684226076\n",
      "loss: 0.44906498246920024\n",
      "loss: 0.4345742285026448\n",
      "loss: 0.44906841696922545\n",
      "loss: 0.4345398544433369\n",
      "loss: 0.4490445325297497\n",
      "loss: 0.4345304363061458\n",
      "loss: 0.4490575015176747\n",
      "loss: 0.4344879938610393\n",
      "loss: 0.44903081505793474\n",
      "loss: 0.4344768399838204\n",
      "loss: 0.4490126989775335\n",
      "loss: 0.4344645854088696\n",
      "loss: 0.44900656545775436\n",
      "loss: 0.43443329326772034\n",
      "loss: 0.4489978362934383\n",
      "loss: 0.4344048188898896\n",
      "loss: 0.4489706446473367\n",
      "loss: 0.4343945383211563\n",
      "loss: 0.4489769307721697\n",
      "loss: 0.43435439599355774\n",
      "loss: 0.4489510201619062\n",
      "loss: 0.43434188038655447\n",
      "loss: 0.4489415207044403\n",
      "loss: 0.4343134209081478\n",
      "loss: 0.4489151376892393\n",
      "loss: 0.4343014137148456\n",
      "loss: 0.44890042238377714\n",
      "loss: 0.4342815528665825\n",
      "loss: 0.448881955879779\n",
      "loss: 0.4342630598876709\n",
      "loss: 0.4488681350001909\n",
      "loss: 0.4342418427461399\n",
      "loss: 0.44884609059109637\n",
      "loss: 0.43422920319356606\n",
      "loss: 0.44884674634298966\n",
      "loss: 0.4341966341656509\n",
      "loss: 0.44881485729678383\n",
      "loss: 0.43419629506617424\n",
      "loss: 0.44882563449635254\n",
      "loss: 0.43415387779036807\n",
      "loss: 0.4488013856730989\n",
      "loss: 0.43414450936593524\n",
      "loss: 0.4487917665338776\n",
      "loss: 0.4341214652897006\n",
      "loss: 0.44876872533149953\n",
      "loss: 0.43411314323156364\n",
      "loss: 0.4487782080359126\n",
      "loss: 0.4340715315971811\n",
      "loss: 0.4487623251455197\n",
      "loss: 0.4340545363767309\n",
      "loss: 0.448749417202964\n",
      "loss: 0.4340334847642788\n",
      "loss: 0.44872155190300966\n",
      "loss: 0.43402971849887273\n",
      "loss: 0.4487442499207456\n",
      "loss: 0.4339770702675845\n",
      "loss: 0.448715763722938\n",
      "loss: 0.43397356801364223\n",
      "loss: 0.4487034396717954\n",
      "loss: 0.43395259110738066\n",
      "loss: 0.44868640883248107\n",
      "loss: 0.4339388184995194\n",
      "loss: 0.4486813321116594\n",
      "loss: 0.4339127558541032\n",
      "loss: 0.4486687063045268\n",
      "loss: 0.4338949464692677\n",
      "loss: 0.448662678787772\n",
      "loss: 0.4338709113017016\n",
      "loss: 0.44865173059825064\n",
      "loss: 0.43385176431563266\n",
      "loss: 0.448631572678155\n",
      "loss: 0.4338403527188223\n",
      "loss: 0.4486260135477559\n",
      "loss: 0.43381590973536516\n",
      "loss: 0.4486268536560896\n",
      "loss: 0.4337878150721271\n",
      "loss: 0.44860865657783466\n",
      "loss: 0.4337735599129632\n",
      "loss: 0.44858479951357055\n",
      "loss: 0.43376752543828173\n",
      "loss: 0.4485806805999328\n",
      "loss: 0.4337433125107606\n",
      "loss: 0.4485575466569923\n",
      "loss: 0.43373976017253857\n",
      "loss: 0.4485723264510689\n",
      "loss: 0.4336975448159179\n",
      "loss: 0.44855498099183794\n",
      "loss: 0.4336878963614781\n",
      "loss: 0.4485407040850241\n",
      "loss: 0.4336754708186982\n",
      "loss: 0.44852167088318146\n",
      "loss: 0.43366814044437574\n",
      "loss: 0.4485184692606087\n",
      "loss: 0.4336442889712197\n",
      "loss: 0.44849704687133024\n",
      "loss: 0.43363947733898967\n",
      "loss: 0.44849355982783645\n",
      "loss: 0.4336164258668255\n",
      "loss: 0.44847322431203657\n",
      "loss: 0.4336104179678009\n",
      "loss: 0.44847135484648604\n",
      "loss: 0.43358642444120965\n",
      "loss: 0.44845653468100666\n",
      "loss: 0.4335752815738961\n",
      "loss: 0.44844817129081155\n",
      "loss: 0.4335577579825404\n",
      "loss: 0.44842807381523564\n",
      "loss: 0.43355213896909167\n",
      "loss: 0.4484268007400944\n",
      "loss: 0.4335274923480037\n",
      "loss: 0.4484311231323244\n",
      "loss: 0.4335010950649556\n",
      "loss: 0.44841695157397254\n",
      "loss: 0.43348844688911103\n",
      "loss: 0.4484031263284739\n",
      "loss: 0.4334775843003082\n",
      "loss: 0.4483949566074273\n",
      "loss: 0.4334583953154917\n",
      "loss: 0.44838373246399255\n",
      "loss: 0.43344735513184107\n",
      "loss: 0.44837906794995025\n",
      "loss: 0.4334264606680552\n",
      "loss: 0.4483606035178723\n",
      "loss: 0.433420848277298\n",
      "loss: 0.44836007632733227\n",
      "loss: 0.43339685606570727\n",
      "loss: 0.4483540046707579\n",
      "loss: 0.43337985873883494\n",
      "loss: 0.44834088849184617\n",
      "loss: 0.43336964487100366\n",
      "loss: 0.4483431116703015\n",
      "loss: 0.4333462045144166\n",
      "loss: 0.44833043061553896\n",
      "loss: 0.433335290892776\n",
      "loss: 0.44832691700402233\n",
      "loss: 0.43331743688767865\n",
      "loss: 0.4483416132174944\n",
      "loss: 0.43328307370392694\n",
      "loss: 0.4483309196800329\n",
      "loss: 0.43327085976084706\n",
      "loss: 0.4483196207356931\n",
      "loss: 0.4332595543845661\n",
      "loss: 0.44831434971906065\n",
      "loss: 0.4332425271911644\n",
      "loss: 0.44830553278495106\n",
      "loss: 0.4332296646443643\n",
      "loss: 0.44830686466530134\n",
      "loss: 0.4332078983964252\n",
      "loss: 0.4482947323743825\n",
      "loss: 0.43319826537704603\n",
      "loss: 0.4482956857676333\n",
      "loss: 0.4331768832874933\n",
      "loss: 0.4482882744873133\n",
      "loss: 0.43316310365813815\n",
      "loss: 0.4482872091372798\n",
      "loss: 0.43314254140428443\n",
      "loss: 0.4482786607022064\n",
      "loss: 0.4331323783627077\n",
      "loss: 0.4482770366257147\n",
      "loss: 0.433110262983813\n",
      "loss: 0.4482834328058768\n",
      "loss: 0.4330894121276896\n",
      "loss: 0.4482663691136808\n",
      "loss: 0.4330811867895256\n",
      "loss: 0.44826225437260203\n",
      "loss: 0.4330649542074535\n",
      "loss: 0.4482564179031323\n",
      "loss: 0.43305009543587497\n",
      "loss: 0.44825518683171023\n",
      "loss: 0.43303100126780986\n",
      "loss: 0.448256168569136\n",
      "loss: 0.4330152088533432\n",
      "loss: 0.44826381212477034\n",
      "loss: 0.43298476974194916\n",
      "loss: 0.4482578676956366\n",
      "loss: 0.43296987687902183\n",
      "loss: 0.4482597007662515\n",
      "loss: 0.4329489670145043\n",
      "loss: 0.4482398974092372\n",
      "loss: 0.43294798159875725\n",
      "loss: 0.44824492308692415\n",
      "loss: 0.43292766002875144\n",
      "loss: 0.4482303810175715\n",
      "loss: 0.43292004804830314\n",
      "loss: 0.4482280486623285\n",
      "loss: 0.4329034921711536\n",
      "loss: 0.4482284013810165\n",
      "loss: 0.4328850872914076\n",
      "loss: 0.44821608461272733\n",
      "loss: 0.43287905243305236\n",
      "loss: 0.44821807901544825\n",
      "loss: 0.43285956050479335\n",
      "loss: 0.4482136395777158\n",
      "loss: 0.4328407925618248\n",
      "loss: 0.4482031560299839\n",
      "loss: 0.4328307704512495\n",
      "loss: 0.4481977395434346\n",
      "loss: 0.4328157615329195\n",
      "loss: 0.44817293166801314\n",
      "loss: 0.4328198967391432\n",
      "loss: 0.4481924572062112\n",
      "loss: 0.43277873405914363\n",
      "loss: 0.44817992579602306\n",
      "loss: 0.4327711627523405\n",
      "loss: 0.4481573987210203\n",
      "loss: 0.4327676854684808\n",
      "loss: 0.4481495719788277\n",
      "loss: 0.4327546459647677\n",
      "loss: 0.4481382919988778\n",
      "loss: 0.4327407745906458\n",
      "loss: 0.4481214736026214\n",
      "loss: 0.4327370935183981\n",
      "loss: 0.44812999460548975\n",
      "loss: 0.4327082146668019\n",
      "loss: 0.44811046282835\n",
      "loss: 0.43270145796866694\n",
      "loss: 0.4480986757167884\n",
      "loss: 0.43269189828095317\n",
      "loss: 0.448087455086225\n",
      "loss: 0.4326780735963591\n",
      "loss: 0.44808870891394753\n",
      "loss: 0.43265691143095925\n",
      "loss: 0.44806168315063216\n",
      "loss: 0.43265874160783546\n",
      "loss: 0.44807500568990666\n",
      "loss: 0.43262842003173163\n",
      "loss: 0.44805469095497064\n",
      "loss: 0.4326245267395479\n",
      "loss: 0.44804356064564993\n",
      "loss: 0.43261172912802226\n",
      "loss: 0.4480373918671721\n",
      "loss: 0.43259697448978696\n",
      "loss: 0.44801826841254194\n",
      "loss: 0.4325929558474426\n",
      "loss: 0.44800871812830856\n",
      "loss: 0.43257914034587147\n",
      "loss: 0.44801523562711126\n",
      "loss: 0.4325542767150968\n",
      "loss: 0.4479937845358627\n",
      "loss: 0.43255519859906283\n",
      "loss: 0.44801282316688457\n",
      "loss: 0.43251652787061434\n",
      "loss: 0.44800660914515594\n",
      "loss: 0.43249943598981166\n",
      "loss: 0.4479937508512385\n",
      "loss: 0.43249656015076926\n",
      "loss: 0.44799034952796013\n",
      "loss: 0.43248573996798056\n",
      "loss: 0.4479682371361509\n",
      "loss: 0.4324798848137805\n",
      "loss: 0.44797158415632254\n",
      "loss: 0.43246334586975954\n",
      "loss: 0.44797181472239495\n",
      "loss: 0.43244380070594673\n",
      "loss: 0.44797537056241277\n",
      "loss: 0.43241598329014175\n",
      "loss: 0.44799561862201376\n",
      "loss: 0.4323654127754064\n",
      "loss: 0.4480133122413895\n",
      "loss: 0.43232147172052476\n",
      "loss: 0.4480196584986773\n",
      "loss: 0.43229358379307065\n",
      "loss: 0.44802834142105175\n",
      "loss: 0.4322648904851709\n",
      "loss: 0.44803395433486215\n",
      "loss: 0.4322277856875261\n",
      "loss: 0.44804544340852737\n",
      "loss: 0.43220979622542705\n",
      "loss: 0.44807035360943187\n",
      "loss: 0.43215760842273676\n",
      "loss: 0.4480641703887778\n",
      "loss: 0.43213897111617977\n",
      "loss: 0.4480683562896844\n",
      "loss: 0.4321170755708636\n",
      "loss: 0.4480771813150402\n",
      "loss: 0.4320821080480981\n",
      "loss: 0.4480771832721688\n",
      "loss: 0.43206259646562295\n",
      "loss: 0.44806597163962947\n",
      "loss: 0.43204616588481154\n",
      "loss: 0.44809551004157\n",
      "loss: 0.43200127116808495\n",
      "loss: 0.4480807498066419\n",
      "loss: 0.43198809495543855\n",
      "loss: 0.4480831306132768\n",
      "loss: 0.4319668229106823\n",
      "loss: 0.4480736251280725\n",
      "loss: 0.43195077377068675\n",
      "loss: 0.44809669119399753\n",
      "loss: 0.43191176714905405\n",
      "loss: 0.44808869643714155\n",
      "loss: 0.4318920258891647\n",
      "loss: 0.44808875593821845\n",
      "loss: 0.43187569567645356\n",
      "loss: 0.44807355471907656\n",
      "loss: 0.4318654548361807\n",
      "loss: 0.44810306818387213\n",
      "loss: 0.43182083511387614\n",
      "loss: 0.44808786192793865\n",
      "loss: 0.431808231442541\n",
      "loss: 0.4480879875040283\n",
      "loss: 0.4317924394313594\n",
      "loss: 0.4480770366579895\n",
      "loss: 0.4317793182839626\n",
      "loss: 0.44809769682605893\n",
      "loss: 0.4317434549547088\n",
      "loss: 0.44808525894768436\n",
      "loss: 0.43172763119248087\n",
      "loss: 0.44808471229158625\n",
      "loss: 0.4317168535117995\n",
      "loss: 0.44810252063508493\n",
      "loss: 0.431675975180323\n",
      "loss: 0.44809346267183664\n",
      "loss: 0.4316676218430968\n",
      "loss: 0.44807998054656734\n",
      "loss: 0.4316560021760459\n",
      "loss: 0.44807754652173\n",
      "loss: 0.4316407215710608\n",
      "loss: 0.4480947843291323\n",
      "loss: 0.43160742821903375\n",
      "loss: 0.44808025037834937\n",
      "loss: 0.431597299159466\n",
      "loss: 0.4480689925377135\n",
      "loss: 0.431590196560902\n",
      "loss: 0.4480849924885202\n",
      "loss: 0.4315538000416117\n",
      "loss: 0.4480730153130968\n",
      "loss: 0.4315481585415779\n",
      "loss: 0.4480664243108777\n",
      "loss: 0.4315318706569635\n",
      "loss: 0.44806908709218424\n",
      "loss: 0.43151566884538856\n",
      "loss: 0.4480531244262666\n",
      "loss: 0.4315059191926735\n",
      "loss: 0.44803952018614723\n",
      "loss: 0.4315025095545683\n",
      "loss: 0.4480524939525994\n",
      "loss: 0.43146862625018184\n",
      "loss: 0.4480352642430166\n",
      "loss: 0.43146960130893575\n",
      "loss: 0.448025519641673\n",
      "loss: 0.4314523735481499\n",
      "loss: 0.44800059877795917\n",
      "loss: 0.43146081139719844\n",
      "loss: 0.44801816847098314\n",
      "loss: 0.4314242780801639\n",
      "loss: 0.44800324808493175\n",
      "loss: 0.43141339945045254\n",
      "loss: 0.44798758073375794\n",
      "loss: 0.43141564229071594\n",
      "loss: 0.448004131791513\n",
      "loss: 0.4313763580369589\n",
      "loss: 0.4479862928196029\n",
      "loss: 0.43137711369237836\n",
      "loss: 0.44797222674341874\n",
      "loss: 0.43136619267707504\n",
      "loss: 0.4479643264241882\n",
      "loss: 0.43136194947331735\n",
      "loss: 0.4479689955845037\n",
      "loss: 0.431331720786079\n",
      "loss: 0.44794801807812434\n",
      "loss: 0.43133854494671164\n",
      "loss: 0.4479678553552431\n",
      "loss: 0.43129706460191475\n",
      "loss: 0.4479567987079229\n",
      "loss: 0.431297771935514\n",
      "loss: 0.44792696719803105\n",
      "loss: 0.4312998260515678\n",
      "loss: 0.4479493589619473\n",
      "loss: 0.4312638726297775\n",
      "loss: 0.4479162135525718\n",
      "loss: 0.4312720495608852\n",
      "loss: 0.4479089120799013\n",
      "loss: 0.43126352267711504\n",
      "loss: 0.4479148406154514\n",
      "loss: 0.4312459757468258\n",
      "loss: 0.44790487455275513\n",
      "loss: 0.43122597683381114\n",
      "loss: 0.4478860490188817\n",
      "loss: 0.4312317395426065\n",
      "loss: 0.4478978635794927\n",
      "loss: 0.4311979368941397\n",
      "loss: 0.4478865085932284\n",
      "loss: 0.4311973070484165\n",
      "loss: 0.44786640671635763\n",
      "loss: 0.43118511520612823\n",
      "loss: 0.4478817936834312\n",
      "loss: 0.4311619166270162\n",
      "loss: 0.4478627240369336\n",
      "loss: 0.43115376518200976\n",
      "loss: 0.4478508573883748\n",
      "loss: 0.43114558951178794\n",
      "loss: 0.4478548239885043\n",
      "loss: 0.43112429936312024\n",
      "loss: 0.4478385231622501\n",
      "loss: 0.43111905287091173\n",
      "loss: 0.4478213693418384\n",
      "loss: 0.4311150998100864\n",
      "loss: 0.4478340670536719\n",
      "loss: 0.4310848951930977\n",
      "loss: 0.447812333935036\n",
      "loss: 0.4310851797325808\n",
      "loss: 0.44780503780438063\n",
      "loss: 0.4310693006467519\n",
      "loss: 0.44781098860097335\n",
      "loss: 0.4310479947870944\n",
      "loss: 0.4477927881430861\n",
      "loss: 0.4310461560055581\n",
      "loss: 0.4477728917476657\n",
      "loss: 0.4310455698006232\n",
      "loss: 0.44779006895693724\n",
      "loss: 0.43101038342798176\n",
      "loss: 0.4477734644027249\n",
      "loss: 0.43100483763336783\n",
      "loss: 0.44774820698287054\n",
      "loss: 0.4309990946890843\n",
      "loss: 0.447747641159575\n",
      "loss: 0.43098774736556966\n",
      "loss: 0.4477544955363148\n",
      "loss: 0.4309577247861121\n",
      "loss: 0.4477483050985699\n",
      "loss: 0.43094067893922416\n",
      "loss: 0.4477353140386851\n",
      "loss: 0.43093726750523065\n",
      "loss: 0.4477262430471386\n",
      "loss: 0.4309262583118062\n",
      "loss: 0.44772880440428225\n",
      "loss: 0.4308955991465214\n",
      "loss: 0.4477264541308136\n",
      "loss: 0.4308852977577938\n",
      "loss: 0.4477089577946833\n",
      "loss: 0.4308787138485343\n",
      "loss: 0.44770372025371136\n",
      "loss: 0.43085998604939635\n",
      "loss: 0.4476949323758634\n",
      "loss: 0.4308526230129623\n",
      "loss: 0.4477093701369805\n",
      "loss: 0.4308196180641654\n",
      "loss: 0.447690546699362\n",
      "loss: 0.43082298045255685\n",
      "loss: 0.44767356130055164\n",
      "loss: 0.4308101855374422\n",
      "loss: 0.4476620651201814\n",
      "loss: 0.4307987033192726\n",
      "loss: 0.44766304375899263\n",
      "loss: 0.4307819812085679\n",
      "loss: 0.44767206681330357\n",
      "loss: 0.4307552977718863\n",
      "loss: 0.4476580424804265\n",
      "loss: 0.43074215380418585\n",
      "loss: 0.44764708441468093\n",
      "loss: 0.430725841604323\n",
      "loss: 0.44764480445286864\n",
      "loss: 0.43070729595939783\n",
      "loss: 0.44763043256095064\n",
      "loss: 0.430694891427495\n",
      "loss: 0.4476498600721646\n",
      "loss: 0.43065513576741743\n",
      "loss: 0.4476192099096834\n",
      "loss: 0.4306490062158616\n",
      "loss: 0.447613965063499\n",
      "loss: 0.4306331476581334\n",
      "loss: 0.4476231589327189\n",
      "loss: 0.43060292773098896\n",
      "loss: 0.44760403684343925\n",
      "loss: 0.43058788399965914\n",
      "loss: 0.4476095993999659\n",
      "loss: 0.4305590831005275\n",
      "loss: 0.44758516444735824\n",
      "loss: 0.4305565635651724\n",
      "loss: 0.4475722455609091\n",
      "loss: 0.43053549439132605\n",
      "loss: 0.44756809256570323\n",
      "loss: 0.4305037985377039\n",
      "loss: 0.44753505219034967\n",
      "loss: 0.43050835730700215\n",
      "loss: 0.4475359017278183\n",
      "loss: 0.4304747345330104\n",
      "loss: 0.4474984523774636\n",
      "loss: 0.4304834531122798\n",
      "loss: 0.44749520097586104\n",
      "loss: 0.4304529054258931\n",
      "loss: 0.4474905119922496\n",
      "loss: 0.43043440416653217\n",
      "loss: 0.4474592863908819\n",
      "loss: 0.4304328412437034\n",
      "loss: 0.4474528988932982\n",
      "loss: 0.43040830860896234\n",
      "loss: 0.44742516383984193\n",
      "loss: 0.4304135793788901\n",
      "loss: 0.4474151992907998\n",
      "loss: 0.43039943016145704\n",
      "loss: 0.4474077646613658\n",
      "loss: 0.43037121110095505\n",
      "loss: 0.4473786935129184\n",
      "loss: 0.4303741349633993\n",
      "loss: 0.44737757888903373\n",
      "loss: 0.4303488359529244\n",
      "loss: 0.4473427692282112\n",
      "loss: 0.43036193095252734\n",
      "loss: 0.447340333301928\n",
      "loss: 0.43033111281470404\n",
      "loss: 0.44733836916850095\n",
      "loss: 0.4303157009537562\n",
      "loss: 0.4473019436004699\n",
      "loss: 0.4303105785144596\n",
      "loss: 0.447316895176999\n",
      "loss: 0.43028325989849686\n",
      "loss: 0.44728024217605034\n",
      "loss: 0.430289597708829\n",
      "loss: 0.44727594638443435\n",
      "loss: 0.43026307496486077\n",
      "loss: 0.44726958839479025\n",
      "loss: 0.4302372573150865\n",
      "loss: 0.4472337403404284\n",
      "loss: 0.43025257567686404\n",
      "loss: 0.4472288799647946\n",
      "loss: 0.43021695323028525\n",
      "loss: 0.4471973636425581\n",
      "loss: 0.43023341024629824\n",
      "loss: 0.4472012720890374\n",
      "loss: 0.4302051706782299\n",
      "loss: 0.4471633501376772\n",
      "loss: 0.4302067704233546\n",
      "loss: 0.447168246123772\n",
      "loss: 0.4301778214028944\n",
      "loss: 0.44716711531973463\n",
      "loss: 0.4301611843425685\n",
      "loss: 0.4471258556649879\n",
      "loss: 0.43016870642811\n",
      "loss: 0.4471070008639816\n",
      "loss: 0.43014843247975376\n",
      "loss: 0.44708315615437155\n",
      "loss: 0.4301529293237866\n",
      "loss: 0.44706436733774985\n",
      "loss: 0.43013785676720967\n",
      "loss: 0.44703150771643074\n",
      "loss: 0.43014066711864124\n",
      "loss: 0.44701664811810315\n",
      "loss: 0.43012386741440267\n",
      "loss: 0.44700056497707263\n",
      "loss: 0.4301136414450899\n",
      "loss: 0.44696012082894726\n",
      "loss: 0.4301089125117618\n",
      "loss: 0.44694234059089516\n",
      "loss: 0.43009869056951805\n",
      "loss: 0.44689654522304456\n",
      "loss: 0.43010648910691174\n",
      "loss: 0.44687878897253364\n",
      "loss: 0.4300895679767475\n",
      "loss: 0.44686066621850834\n",
      "loss: 0.4300755966851269\n",
      "loss: 0.44680924649159387\n",
      "loss: 0.4300874184663866\n",
      "loss: 0.44679366305851087\n",
      "loss: 0.4300630660602822\n",
      "loss: 0.4467822296536856\n",
      "loss: 0.4300417873070914\n",
      "loss: 0.4467375137648306\n",
      "loss: 0.43004605654157646\n",
      "loss: 0.446723140843104\n",
      "loss: 0.4300146379960584\n",
      "loss: 0.44668715531592307\n",
      "loss: 0.43001006056232055\n",
      "loss: 0.44665971582353226\n",
      "loss: 0.4300012606001074\n",
      "loss: 0.446642196104872\n",
      "loss: 0.4299790597691793\n",
      "loss: 0.4466069110290803\n",
      "loss: 0.4299787856641243\n",
      "loss: 0.44656037162329293\n",
      "loss: 0.4299763000724297\n",
      "loss: 0.44652253577489\n",
      "loss: 0.4299789096408176\n",
      "loss: 0.44651690249302595\n",
      "loss: 0.42994715691517776\n",
      "loss: 0.44647134953646794\n",
      "loss: 0.4299446476270639\n",
      "loss: 0.446439194241211\n",
      "loss: 0.4299418323715559\n",
      "loss: 0.44642020186915565\n",
      "loss: 0.42991538417252134\n",
      "loss: 0.4463927198824612\n",
      "loss: 0.4299080829373843\n",
      "loss: 0.446352527893809\n",
      "loss: 0.4299089593902643\n",
      "loss: 0.4463305509840694\n",
      "loss: 0.4298821599743459\n",
      "loss: 0.44630558469674153\n",
      "loss: 0.4298784998508417\n",
      "loss: 0.4462607088251681\n",
      "loss: 0.4298749407092628\n",
      "loss: 0.4462381456660183\n",
      "loss: 0.42986261756036265\n",
      "loss: 0.44622217261142966\n",
      "loss: 0.4298448841452542\n",
      "loss: 0.4461760006467959\n",
      "loss: 0.4298411549120941\n",
      "loss: 0.44615295843679686\n",
      "loss: 0.4298293613074968\n",
      "loss: 0.4461445262941415\n",
      "loss: 0.4298081800416501\n",
      "loss: 0.44611024654197295\n",
      "loss: 0.4298010478462976\n",
      "loss: 0.446100584752102\n",
      "loss: 0.42977540624334787\n",
      "loss: 0.446086005341973\n",
      "loss: 0.42975301814381633\n",
      "loss: 0.4460624536483857\n",
      "loss: 0.42974791822792385\n",
      "loss: 0.4460229177940315\n",
      "loss: 0.4297373151319151\n",
      "loss: 0.44603047314281397\n",
      "loss: 0.4297051143912468\n",
      "loss: 0.4459909820761512\n",
      "loss: 0.4296877474478916\n",
      "loss: 0.44597812590273017\n",
      "loss: 0.42968115859551137\n",
      "loss: 0.4459707607893421\n",
      "loss: 0.429654900773843\n",
      "loss: 0.4459362939701669\n",
      "loss: 0.4296402341402332\n",
      "loss: 0.445939790989346\n",
      "loss: 0.42961488878052395\n",
      "loss: 0.44589584845500047\n",
      "loss: 0.42960634075089016\n",
      "loss: 0.44589451813572956\n",
      "loss: 0.4295720512059045\n",
      "loss: 0.44587173904110816\n",
      "loss: 0.4295570150501401\n",
      "loss: 0.44584024589399857\n",
      "loss: 0.42953942078167745\n",
      "loss: 0.4458444450718709\n",
      "loss: 0.4295024818385293\n",
      "loss: 0.44582547319017773\n",
      "loss: 0.429491554836678\n",
      "loss: 0.4458096355807713\n",
      "loss: 0.429472634957036\n",
      "loss: 0.4457791679999602\n",
      "loss: 0.42947667223741964\n",
      "loss: 0.4457280170321857\n",
      "loss: 0.4294815619063674\n",
      "loss: 0.4457260031668616\n",
      "loss: 0.4294549043869607\n",
      "loss: 0.4456661550442804\n",
      "loss: 0.4294626694505608\n",
      "loss: 0.4456663314891145\n",
      "loss: 0.42943825947225256\n",
      "loss: 0.44561596366515516\n",
      "loss: 0.4294437251184678\n",
      "loss: 0.445583332694861\n",
      "loss: 0.4294439655971059\n",
      "loss: 0.445553287558207\n",
      "loss: 0.4294306215946356\n",
      "loss: 0.4455167611802149\n",
      "loss: 0.4294239624264146\n",
      "loss: 0.4454654955283912\n",
      "loss: 0.4294283223576161\n",
      "loss: 0.44544476284018475\n",
      "loss: 0.42938965480314606\n",
      "loss: 0.4453998246411577\n",
      "loss: 0.4293857971651006\n",
      "loss: 0.44534073342279085\n",
      "loss: 0.42938445608057246\n",
      "loss: 0.44532908138229194\n",
      "loss: 0.42934622508512954\n",
      "loss: 0.44527650618847614\n",
      "loss: 0.42933742462378444\n",
      "loss: 0.44524131291248426\n",
      "loss: 0.42931937431123474\n",
      "loss: 0.44521124711217086\n",
      "loss: 0.4292819541683033\n",
      "loss: 0.4451431950650465\n",
      "loss: 0.4292698980320905\n",
      "loss: 0.4450998243338662\n",
      "loss: 0.42924514311157413\n",
      "loss: 0.44504662508583176\n",
      "loss: 0.4292430946729998\n",
      "loss: 0.4450177219456806\n",
      "loss: 0.42920142361504493\n",
      "loss: 0.4449702051708455\n",
      "loss: 0.42918386818078424\n",
      "loss: 0.4449060429516333\n",
      "loss: 0.4291740832694503\n",
      "loss: 0.44488203899049467\n",
      "loss: 0.4291369908380442\n",
      "loss: 0.44482193787388546\n",
      "loss: 0.42911469157495824\n",
      "loss: 0.4447623698615327\n",
      "loss: 0.4291095525454552\n",
      "loss: 0.44472629295223437\n",
      "loss: 0.4290642136255231\n",
      "loss: 0.4446682335631317\n",
      "loss: 0.42904824353720833\n",
      "loss: 0.4446125182797663\n",
      "loss: 0.42903758262193686\n",
      "loss: 0.44455276681991096\n",
      "loss: 0.4290186796606793\n",
      "loss: 0.4445091018627409\n",
      "loss: 0.4290058039987884\n",
      "loss: 0.4444623001225533\n",
      "loss: 0.4289825925835637\n",
      "loss: 0.4444229842091995\n",
      "loss: 0.4289567985124529\n",
      "loss: 0.4443815810706146\n",
      "loss: 0.4289545775711015\n",
      "loss: 0.4443090698151127\n",
      "loss: 0.4289464990595523\n",
      "loss: 0.4442611642471402\n",
      "loss: 0.4289248522950325\n",
      "loss: 0.44421167616381024\n",
      "loss: 0.4289085685539339\n",
      "loss: 0.4441638818465639\n",
      "loss: 0.4288915146317175\n",
      "loss: 0.44409618529025463\n",
      "loss: 0.4288740548849977\n",
      "loss: 0.44406413219094776\n",
      "loss: 0.4288508188758786\n",
      "loss: 0.4440008489735472\n",
      "loss: 0.42883267107990514\n",
      "loss: 0.4439496116349475\n",
      "loss: 0.4288154292862979\n",
      "loss: 0.44389660554361665\n",
      "loss: 0.42878750657627174\n",
      "loss: 0.44384923566228596\n",
      "loss: 0.42877007128029665\n",
      "loss: 0.4437814216809577\n",
      "loss: 0.4287580267401416\n",
      "loss: 0.4437220876924234\n",
      "loss: 0.4287280561915091\n",
      "loss: 0.4436723250936131\n",
      "loss: 0.4287166618459185\n",
      "loss: 0.4436226435111154\n",
      "loss: 0.42868810521679224\n",
      "loss: 0.44356927856652534\n",
      "loss: 0.4286550242478414\n",
      "loss: 0.4435121577392181\n",
      "loss: 0.4286464469679368\n",
      "loss: 0.4434584778882569\n",
      "loss: 0.42862207812394804\n",
      "loss: 0.4433979552887762\n",
      "loss: 0.4286143586987266\n",
      "loss: 0.44335106201467184\n",
      "loss: 0.42857642104028354\n",
      "loss: 0.44330886602156155\n",
      "loss: 0.4285584707588175\n",
      "loss: 0.4432683172802492\n",
      "loss: 0.42852618016825283\n",
      "loss: 0.443221701365234\n",
      "loss: 0.4285028120242768\n",
      "loss: 0.4431700771097958\n",
      "loss: 0.42849594544478664\n",
      "loss: 0.44310933550916065\n",
      "loss: 0.4284773285177377\n",
      "loss: 0.4430740520268651\n",
      "loss: 0.428453831125816\n",
      "loss: 0.44302325873919807\n",
      "loss: 0.4284345141333893\n",
      "loss: 0.442994519119903\n",
      "loss: 0.4283885812808607\n",
      "loss: 0.44295527603861534\n",
      "loss: 0.428365833624921\n",
      "loss: 0.4429032726618801\n",
      "loss: 0.4283472910121038\n",
      "loss: 0.44284430120763185\n",
      "loss: 0.4284337896376759\n",
      "loss: 0.4426672640183636\n",
      "loss: 0.4284789400962749\n",
      "loss: 0.442505317999128\n",
      "loss: 0.42850041909193526\n",
      "loss: 0.4423597263266813\n",
      "loss: 0.4285176800577266\n",
      "loss: 0.44221877189252684\n",
      "loss: 0.42853340596055667\n",
      "loss: 0.44207288334021666\n",
      "loss: 0.42854487500808\n",
      "loss: 0.44193784720052864\n",
      "loss: 0.42855004666881086\n",
      "loss: 0.44179694502555344\n",
      "loss: 0.4285623781776558\n",
      "loss: 0.44167602412727863\n",
      "loss: 0.4285541957594783\n",
      "loss: 0.44154231149114687\n",
      "loss: 0.4285693888601225\n",
      "loss: 0.44141789355223543\n",
      "loss: 0.428566057640238\n",
      "loss: 0.44128629260774593\n",
      "loss: 0.428569535442238\n",
      "loss: 0.4411700538860535\n",
      "loss: 0.42857138945249384\n",
      "loss: 0.44103778181746606\n",
      "loss: 0.4285896214740208\n",
      "loss: 0.44091324552595607\n",
      "loss: 0.4285993933041769\n",
      "loss: 0.44079845060866124\n",
      "loss: 0.428604521568247\n",
      "loss: 0.4406874050329106\n",
      "loss: 0.4286128622868522\n",
      "loss: 0.44058362182844346\n",
      "loss: 0.4286121037898525\n",
      "loss: 0.4404655945804877\n",
      "loss: 0.42861402457434983\n",
      "loss: 0.4403600908244475\n",
      "loss: 0.4286147973040095\n",
      "loss: 0.4402565480724249\n",
      "loss: 0.428605607255155\n",
      "loss: 0.4401486803713774\n",
      "loss: 0.428604795015739\n",
      "loss: 0.4400533453598978\n",
      "loss: 0.42858880889523737\n",
      "loss: 0.4399487590536352\n",
      "loss: 0.42858439188103603\n",
      "loss: 0.4398438911200415\n",
      "loss: 0.42857841704307836\n",
      "loss: 0.4397592025976681\n",
      "loss: 0.42854824031235333\n",
      "loss: 0.43967322255339447\n",
      "loss: 0.42853654105752537\n",
      "loss: 0.43960243309808844\n",
      "loss: 0.42850786844700123\n",
      "loss: 0.4395510720182013\n",
      "loss: 0.4284718501208344\n",
      "loss: 0.4395108360968773\n",
      "loss: 0.42842701784773357\n",
      "loss: 0.4394429750743804\n",
      "loss: 0.42841583357693275\n",
      "loss: 0.43941186170707447\n",
      "loss: 0.4283598024495632\n",
      "loss: 0.4393714348795753\n",
      "loss: 0.4283144751685518\n",
      "loss: 0.4393307120505884\n",
      "loss: 0.42827395021201536\n",
      "loss: 0.4392969856714488\n",
      "loss: 0.42826867882355235\n",
      "loss: 0.43919114345166704\n",
      "loss: 0.4282548209991999\n",
      "loss: 0.4390938408026196\n",
      "loss: 0.42825633830046256\n",
      "loss: 0.4389855938130506\n",
      "loss: 0.42825497883774993\n",
      "loss: 0.4389173179775111\n",
      "loss: 0.4282323754539179\n",
      "loss: 0.4388456750234378\n",
      "loss: 0.42820094943456616\n",
      "loss: 0.43876986766582565\n",
      "loss: 0.42817777588842626\n",
      "loss: 0.4386976662189977\n",
      "loss: 0.42816011552718275\n",
      "loss: 0.43861814200480315\n",
      "loss: 0.4281291396991964\n",
      "loss: 0.43854119720178014\n",
      "loss: 0.4281065629414305\n",
      "loss: 0.4384369111369643\n",
      "loss: 0.4281064595054311\n",
      "loss: 0.43835778073636744\n",
      "loss: 0.42808345449472307\n",
      "loss: 0.43828628235960065\n",
      "loss: 0.4280502359105153\n",
      "loss: 0.4382088705070189\n",
      "loss: 0.42802547065919405\n",
      "loss: 0.4381082865439831\n",
      "loss: 0.4280277067348641\n",
      "loss: 0.43804120503942495\n",
      "loss: 0.427992848389857\n",
      "loss: 0.43795741680646855\n",
      "loss: 0.42797822582853945\n",
      "loss: 0.4378907034069953\n",
      "loss: 0.4279414606293193\n",
      "loss: 0.43778604860713727\n",
      "loss: 0.42794507975476315\n",
      "loss: 0.4377259321541218\n",
      "loss: 0.4279077354943017\n",
      "loss: 0.4376491375994305\n",
      "loss: 0.4278878716493154\n",
      "loss: 0.4375844114126889\n",
      "loss: 0.4278500841888416\n",
      "loss: 0.43752033506810034\n",
      "loss: 0.42781589050783114\n",
      "loss: 0.43740819252830204\n",
      "loss: 0.4278290451310532\n",
      "loss: 0.43735676771417253\n",
      "loss: 0.4277896128370688\n",
      "loss: 0.4372907433600208\n",
      "loss: 0.4277595581933494\n",
      "loss: 0.43723232823267094\n",
      "loss: 0.4277212470236267\n",
      "loss: 0.4371693869974272\n",
      "loss: 0.42768467907988306\n",
      "loss: 0.43710168356135953\n",
      "loss: 0.4276497843501969\n",
      "loss: 0.43698467935839747\n",
      "loss: 0.4276712469435348\n",
      "loss: 0.4369112497640242\n",
      "loss: 0.4276366484262828\n",
      "loss: 0.4368407425512498\n",
      "loss: 0.42759436560357234\n",
      "loss: 0.4367705346793783\n",
      "loss: 0.4275630388819607\n",
      "loss: 0.4367077314673728\n",
      "loss: 0.4275318493822717\n",
      "loss: 0.4366444273369013\n",
      "loss: 0.42749390966611467\n",
      "loss: 0.4365450586780584\n",
      "loss: 0.42750216863904317\n",
      "loss: 0.4364835013279149\n",
      "loss: 0.42746684866454693\n",
      "loss: 0.43641358912426664\n",
      "loss: 0.4274401618544545\n",
      "loss: 0.436345226538628\n",
      "loss: 0.4274134457408783\n",
      "loss: 0.43629919646115195\n",
      "loss: 0.42737581301927874\n",
      "loss: 0.4362280934766042\n",
      "loss: 0.4273463255074985\n",
      "loss: 0.4361688750183124\n",
      "loss: 0.4273236263614029\n",
      "loss: 0.4361172666790064\n",
      "loss: 0.42727865140199683\n",
      "loss: 0.43605960927526594\n",
      "loss: 0.4272462639522571\n",
      "loss: 0.4360046515129259\n",
      "loss: 0.4272126761177546\n",
      "loss: 0.43595413404988126\n",
      "loss: 0.42716664288072365\n",
      "loss: 0.4358989417545702\n",
      "loss: 0.4271373965301686\n",
      "loss: 0.43583683546620355\n",
      "loss: 0.4271082675034042\n",
      "loss: 0.4357852905682013\n",
      "loss: 0.42706120731783115\n",
      "loss: 0.4357334570645868\n",
      "loss: 0.42703571056023737\n",
      "loss: 0.4356665995624003\n",
      "loss: 0.4270067490968078\n",
      "loss: 0.43561464033731756\n",
      "loss: 0.4269687445481867\n",
      "loss: 0.4355510502839368\n",
      "loss: 0.42693292692511936\n",
      "loss: 0.4354982348744437\n",
      "loss: 0.42689138055603376\n",
      "loss: 0.43545054888372\n",
      "loss: 0.4268446727919709\n",
      "loss: 0.4353927706045433\n",
      "loss: 0.42680533569212753\n",
      "loss: 0.4353416981845473\n",
      "loss: 0.42675890646787373\n",
      "loss: 0.435273779617595\n",
      "loss: 0.42673234819806916\n",
      "loss: 0.4352026227869716\n",
      "loss: 0.4267030830331008\n",
      "loss: 0.43514648195119726\n",
      "loss: 0.4266640634072839\n",
      "loss: 0.4350629188127135\n",
      "loss: 0.42664277957675667\n",
      "loss: 0.43499246093399163\n",
      "loss: 0.42661018096452025\n",
      "loss: 0.43491337179012995\n",
      "loss: 0.4265867208194773\n",
      "loss: 0.43482774618470826\n",
      "loss: 0.4265591903209742\n",
      "loss: 0.434734034542638\n",
      "loss: 0.42654009026191486\n",
      "loss: 0.43465344502615294\n",
      "loss: 0.42650654909056657\n",
      "loss: 0.43455568829463703\n",
      "loss: 0.4264842519472202\n",
      "loss: 0.43447067450536786\n",
      "loss: 0.42645238998532925\n",
      "loss: 0.43437475565409334\n",
      "loss: 0.4264247788538467\n",
      "loss: 0.4342993596158013\n",
      "loss: 0.42640128633944885\n",
      "loss: 0.43421552499809746\n",
      "loss: 0.4263728591302935\n",
      "loss: 0.43412048803894315\n",
      "loss: 0.42636080565567064\n",
      "loss: 0.4340353613134685\n",
      "loss: 0.42632396729107774\n",
      "loss: 0.4339502047294183\n",
      "loss: 0.42630667965075897\n",
      "loss: 0.43388436076307385\n",
      "loss: 0.426263217005143\n",
      "loss: 0.43382669591980544\n",
      "loss: 0.42624344686097093\n",
      "loss: 0.4337354981160868\n",
      "loss: 0.42622111407832025\n",
      "loss: 0.43366869346266923\n",
      "loss: 0.4261980248078731\n",
      "loss: 0.4335932875000239\n",
      "loss: 0.4261642109022507\n",
      "loss: 0.43349909431332107\n",
      "loss: 0.42615739498915767\n",
      "loss: 0.43343420836099994\n",
      "loss: 0.4261370374349911\n",
      "loss: 0.433354223254323\n",
      "loss: 0.4261039131894968\n",
      "loss: 0.4332914651782565\n",
      "loss: 0.42607483028736554\n",
      "loss: 0.43320739995594004\n",
      "loss: 0.4260645884973345\n",
      "loss: 0.4331311912812845\n",
      "loss: 0.42602601862324035\n",
      "loss: 0.4330581311537584\n",
      "loss: 0.4260153626486764\n",
      "loss: 0.43298114220339806\n",
      "loss: 0.425979176670185\n",
      "loss: 0.4328994930789329\n",
      "loss: 0.42595695026454755\n",
      "loss: 0.43282545934707883\n",
      "loss: 0.4259348527478111\n",
      "loss: 0.43274470502702383\n",
      "loss: 0.4259042393207715\n",
      "loss: 0.4326558437793254\n",
      "loss: 0.4258883303467302\n",
      "loss: 0.43257876323033206\n",
      "loss: 0.42586474253245254\n",
      "loss: 0.4324858177762746\n",
      "loss: 0.4258490336653659\n",
      "loss: 0.43240575498843314\n",
      "loss: 0.4258174497705874\n",
      "loss: 0.4323271383331729\n",
      "loss: 0.4258075335561567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.62337662337663"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,307,8)\n",
    "y_train = y_train.reshape(-1,307,1)\n",
    "X_test = X_test.reshape(-1,8)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "model = MLP([8,12,1])\n",
    "for _ in range(1000): # epochs\n",
    "    for X,y in zip(X_train,y_train):\n",
    "        model.train(X,y,lr=0.1)\n",
    "(model.fpass(X_test) == y_test).sum()/y_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhuvan M\\AppData\\Local\\Temp\\ipykernel_18788\\588445444.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32).view(-1,batch_size,8)\n",
      "C:\\Users\\Bhuvan M\\AppData\\Local\\Temp\\ipykernel_18788\\588445444.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).view(-1,batch_size,1)\n",
      "C:\\Users\\Bhuvan M\\AppData\\Local\\Temp\\ipykernel_18788\\588445444.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\Bhuvan M\\AppData\\Local\\Temp\\ipykernel_18788\\588445444.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.6638379693031311\n",
      "Epoch [20/1000], Loss: 0.6441455483436584\n",
      "Epoch [30/1000], Loss: 0.6336659789085388\n",
      "Epoch [40/1000], Loss: 0.6231134533882141\n",
      "Epoch [50/1000], Loss: 0.608777642250061\n",
      "Epoch [60/1000], Loss: 0.5892583727836609\n",
      "Epoch [70/1000], Loss: 0.5670616030693054\n",
      "Epoch [80/1000], Loss: 0.5461388230323792\n",
      "Epoch [90/1000], Loss: 0.5284488797187805\n",
      "Epoch [100/1000], Loss: 0.5141057968139648\n",
      "Epoch [110/1000], Loss: 0.5023951530456543\n",
      "Epoch [120/1000], Loss: 0.4928378164768219\n",
      "Epoch [130/1000], Loss: 0.4853479862213135\n",
      "Epoch [140/1000], Loss: 0.47973328828811646\n",
      "Epoch [150/1000], Loss: 0.47561192512512207\n",
      "Epoch [160/1000], Loss: 0.4725426733493805\n",
      "Epoch [170/1000], Loss: 0.4700946509838104\n",
      "Epoch [180/1000], Loss: 0.46823784708976746\n",
      "Epoch [190/1000], Loss: 0.4666786789894104\n",
      "Epoch [200/1000], Loss: 0.465364933013916\n",
      "Epoch [210/1000], Loss: 0.46414268016815186\n",
      "Epoch [220/1000], Loss: 0.4629319906234741\n",
      "Epoch [230/1000], Loss: 0.46183910965919495\n",
      "Epoch [240/1000], Loss: 0.46070316433906555\n",
      "Epoch [250/1000], Loss: 0.45954620838165283\n",
      "Epoch [260/1000], Loss: 0.45856204628944397\n",
      "Epoch [270/1000], Loss: 0.45758986473083496\n",
      "Epoch [280/1000], Loss: 0.456594318151474\n",
      "Epoch [290/1000], Loss: 0.45560070872306824\n",
      "Epoch [300/1000], Loss: 0.45449569821357727\n",
      "Epoch [310/1000], Loss: 0.45325687527656555\n",
      "Epoch [320/1000], Loss: 0.45188403129577637\n",
      "Epoch [330/1000], Loss: 0.4504295587539673\n",
      "Epoch [340/1000], Loss: 0.44894126057624817\n",
      "Epoch [350/1000], Loss: 0.4474519193172455\n",
      "Epoch [360/1000], Loss: 0.44586560130119324\n",
      "Epoch [370/1000], Loss: 0.44425898790359497\n",
      "Epoch [380/1000], Loss: 0.442446231842041\n",
      "Epoch [390/1000], Loss: 0.44063228368759155\n",
      "Epoch [400/1000], Loss: 0.4387569725513458\n",
      "Epoch [410/1000], Loss: 0.4367351830005646\n",
      "Epoch [420/1000], Loss: 0.43477270007133484\n",
      "Epoch [430/1000], Loss: 0.43283504247665405\n",
      "Epoch [440/1000], Loss: 0.4307956397533417\n",
      "Epoch [450/1000], Loss: 0.42894473671913147\n",
      "Epoch [460/1000], Loss: 0.42687520384788513\n",
      "Epoch [470/1000], Loss: 0.42480194568634033\n",
      "Epoch [480/1000], Loss: 0.4228498339653015\n",
      "Epoch [490/1000], Loss: 0.4211139380931854\n",
      "Epoch [500/1000], Loss: 0.41936302185058594\n",
      "Epoch [510/1000], Loss: 0.4177294671535492\n",
      "Epoch [520/1000], Loss: 0.4162631332874298\n",
      "Epoch [530/1000], Loss: 0.4148811995983124\n",
      "Epoch [540/1000], Loss: 0.4135977625846863\n",
      "Epoch [550/1000], Loss: 0.4123520255088806\n",
      "Epoch [560/1000], Loss: 0.4112458825111389\n",
      "Epoch [570/1000], Loss: 0.4102327823638916\n",
      "Epoch [580/1000], Loss: 0.4092002511024475\n",
      "Epoch [590/1000], Loss: 0.4080796539783478\n",
      "Epoch [600/1000], Loss: 0.4068242013454437\n",
      "Epoch [610/1000], Loss: 0.40573492646217346\n",
      "Epoch [620/1000], Loss: 0.4047298729419708\n",
      "Epoch [630/1000], Loss: 0.40377816557884216\n",
      "Epoch [640/1000], Loss: 0.40258094668388367\n",
      "Epoch [650/1000], Loss: 0.40168067812919617\n",
      "Epoch [660/1000], Loss: 0.4006372392177582\n",
      "Epoch [670/1000], Loss: 0.39966699481010437\n",
      "Epoch [680/1000], Loss: 0.3989395499229431\n",
      "Epoch [690/1000], Loss: 0.39815986156463623\n",
      "Epoch [700/1000], Loss: 0.39753544330596924\n",
      "Epoch [710/1000], Loss: 0.396849125623703\n",
      "Epoch [720/1000], Loss: 0.39601635932922363\n",
      "Epoch [730/1000], Loss: 0.39525866508483887\n",
      "Epoch [740/1000], Loss: 0.3946312963962555\n",
      "Epoch [750/1000], Loss: 0.3938479423522949\n",
      "Epoch [760/1000], Loss: 0.39307764172554016\n",
      "Epoch [770/1000], Loss: 0.3924599289894104\n",
      "Epoch [780/1000], Loss: 0.39183321595191956\n",
      "Epoch [790/1000], Loss: 0.3910773992538452\n",
      "Epoch [800/1000], Loss: 0.3902747333049774\n",
      "Epoch [810/1000], Loss: 0.38967034220695496\n",
      "Epoch [820/1000], Loss: 0.3888532817363739\n",
      "Epoch [830/1000], Loss: 0.38806578516960144\n",
      "Epoch [840/1000], Loss: 0.3871840238571167\n",
      "Epoch [850/1000], Loss: 0.38588422536849976\n",
      "Epoch [860/1000], Loss: 0.3848058581352234\n",
      "Epoch [870/1000], Loss: 0.3836738169193268\n",
      "Epoch [880/1000], Loss: 0.38262948393821716\n",
      "Epoch [890/1000], Loss: 0.3816525638103485\n",
      "Epoch [900/1000], Loss: 0.3809414505958557\n",
      "Epoch [910/1000], Loss: 0.3803456425666809\n",
      "Epoch [920/1000], Loss: 0.37952813506126404\n",
      "Epoch [930/1000], Loss: 0.3789536952972412\n",
      "Epoch [940/1000], Loss: 0.3781485855579376\n",
      "Epoch [950/1000], Loss: 0.3774082362651825\n",
      "Epoch [960/1000], Loss: 0.37669670581817627\n",
      "Epoch [970/1000], Loss: 0.3761623203754425\n",
      "Epoch [980/1000], Loss: 0.37562859058380127\n",
      "Epoch [990/1000], Loss: 0.3749625086784363\n",
      "Epoch [1000/1000], Loss: 0.3742775321006775\n",
      "Test Accuracy: 73.38%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 307\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).view(-1,batch_size,8)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1,batch_size,1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "class MLPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPT, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 12)\n",
    "        self.fc2 = nn.Linear(12, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = MLPT()\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # permutation = torch.randperm(X_train.size()[0])\n",
    "    for batch_X,batch_y in zip(X_train,y_train):\n",
    "        # indices = permutation[i:i+batch_size]\n",
    "        # batch_X, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    accuracy = (predicted == y_test).sum() / y_test.size(0)\n",
    "    print(f'Test Accuracy: {accuracy.item() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ai_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
