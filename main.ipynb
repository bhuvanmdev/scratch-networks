{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) [+-*/] (3, 4) = (3, 4)\n",
      "(3, 1) [+-*/] (1, 4) = (3, 4)\n",
      "(3, 1) [+-*/] (3, 1) = (3, 1)\n",
      "(3, 4) [+-*/] (3, 1) = (3, 4)\n",
      "(3, 4) [+-*/] (1, 4) = (3, 4)\n",
      "(3, 4) [+-*/] (3, 4) = (3, 4)\n"
     ]
    }
   ],
   "source": [
    "# different broadcast rules in numpy\n",
    "shapes = [[(3,1),(3,4)],[(3,1),(1,4)],[(3,1),(3,1)],[(3,4),(3,1)],[(3,4),(1,4)],[(3,4),(3,4)]]\n",
    "for aa,bb in shapes:\n",
    "    a = np.random.randn(*aa)\n",
    "    b = np.random.randn(*bb)\n",
    "    print(a.shape,'[+-*/]',b.shape,'=',(a-b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (100, 1))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = np.random.rand(100).reshape(1,-1)\n",
    "w2 = np.random.rand(100).reshape(-1,1)\n",
    "np.dot(w1,w2)\n",
    "w1.shape,w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple logistic regression from scratch and use gradient descent\n",
    "\n",
    "class LR:\n",
    "    def __init__(self,inp_size=2) -> None:\n",
    "        # z = x1 * w1 + x2 * w2 + b\n",
    "        self.w = np.random.rand(inp_size,1) #  left features X right features[<>]\n",
    "        self.b = np.random.rand(1,1)\n",
    "    \n",
    "    def fpass(self,x):\n",
    "        z = x @ self.w + self.b\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def BCE(self,target,predicted):\n",
    "        return -(target*np.log(predicted) + (1-target)*np.log(1-predicted))\n",
    "    \n",
    "    def train(self,x,y,epoch=100,lr=0.01):\n",
    "        batch = x.shape[0]\n",
    "        for _ in range(1,epoch+1):\n",
    "            if _%10==0:\n",
    "                print(self.w,self.b)\n",
    "            z = x @ self.w + self.b # (z = xw + b)\n",
    "            a = self.sigmoid(z)\n",
    "            loss = self.BCE(y,a)\n",
    "         \n",
    "            da = -(y/a - (1-y)/(1-a))\n",
    "            dz = da * (a - a**2) #dz = a - y\n",
    "            dw = (x.T @ dz)/batch\n",
    "            db = dz.sum(axis=0,keepdims=True)/batch\n",
    "\n",
    "            self.w -= lr*dw\n",
    "            self.b -= lr*db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36002527]\n",
      " [0.93590538]] [[0.70417022]]\n",
      "[[0.34425671]\n",
      " [0.91976212]] [[0.67396657]]\n",
      "[[0.32886769]\n",
      " [0.90394858]] [[0.64451325]]\n",
      "[[0.31385917]\n",
      " [0.88846648]] [[0.6158102]]\n",
      "[[0.29923137]\n",
      " [0.87331686]] [[0.58785601]]\n",
      "[[0.28498377]\n",
      " [0.85850015]] [[0.56064798]]\n",
      "[[0.27111518]\n",
      " [0.84401614]] [[0.53418209]]\n",
      "[[0.25762372]\n",
      " [0.82986402]] [[0.50845315]]\n",
      "[[0.24450688]\n",
      " [0.81604238]] [[0.4834548]]\n",
      "[[0.23176154]\n",
      " [0.80254927]] [[0.4591796]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = LR(2)\n",
    "x = np.random.rand(64,2)\n",
    "y = np.random.rand(64,1)\n",
    "model.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple logistic regression from scratch and use gradient descent\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,layer_sizes=[]) -> None:\n",
    "        # z = x1 * w1 + x2 * w2 + b\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.w = [np.random.rand(layer_sizes[i],layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "        self.b = [np.random.rand(1,layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "\n",
    "    \n",
    "    def fpass(self,x):\n",
    "        z = x @ self.w + self.b\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self,z):\n",
    "        return z*(1-z)\n",
    "    \n",
    "    def relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "    \n",
    "    def relu_prime(self,z):\n",
    "        return np.where(z>0,1,0)\n",
    "    \n",
    "    def tanh(self,z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def BCE(self,target,predicted):\n",
    "        return -(target*np.log(predicted) + (1-target)*np.log(1-predicted))\n",
    "    \n",
    "    def BCE_prime(self,target,predicted):\n",
    "        return -(target/predicted - (1-target)/(1-predicted))\n",
    "    \n",
    "    def train(self,x,y,epoch=100,lr=0.01):\n",
    "        batch = x.shape[0]\n",
    "        for _ in range(1,epoch+1):\n",
    "            if _%10==0:\n",
    "                print(self.w[0],self.b[0])\n",
    "            a = []\n",
    "            z = []\n",
    "            a.append(x)\n",
    "            layers = len(self.layer_sizes)-1\n",
    "            for i in range(layers):\n",
    "                z.append(a[i] @ self.w[i] + self.b[i])\n",
    "                if i == layers-1:\n",
    "                    if self.layer_sizes[i+1] == 1:\n",
    "                        a.append(self.sigmoid(z[i]))\n",
    "                    else:\n",
    "                        a.append(self.relu(z[i]))\n",
    "                else:\n",
    "                    a.append(self.relu(z[i]))\n",
    "            loss = self.BCE(y,a[-1])\n",
    "\n",
    "\n",
    "\n",
    "            # z1 = x @ self.w[0] + self.b[0] # (z = xw + b)--> (batch,inp) @ (inp,hidden) = (batch,hidden)\n",
    "            # a1 = self.relu(z1) # (batch,hidden)\n",
    "            # z2 = a1 @ self.w[1] + self.b[1] # (batch,hidden) @ (hidden,out) = (batch,out)\n",
    "            # a2 = self.sigmoid(z2)\n",
    "            # loss = self.BCE(y,a2)\n",
    "         \n",
    "            \n",
    "            for i in range(layers,-1,-1):\n",
    "                print(i)\n",
    "                if i == layers:\n",
    "                    da = self.BCE_prime(y,a[i])/batch\n",
    "                    dz = da * self.sigmoid_prime(a[i])\n",
    "                else:\n",
    "                    da = dz @ self.w[i].T\n",
    "                    print(da.shape,z[i].shape,i)\n",
    "                    dz = da * self.relu_prime(z[i])\n",
    "                dw = (a[i-1].T @ dz)\n",
    "                db = dz.sum(axis=0,keepdims=True)\n",
    "                \n",
    "                self.w[i-1] -= lr*dw\n",
    "                self.b[i-1] -= lr*db\n",
    "\n",
    "            # da2 = self.BCE_prime(y,a2)/batch\n",
    "            # dz2 = da2 * self.sigmoid_prime(a2) #dz = a - y\n",
    "            # dw2 = (a1.T @ dz2)\n",
    "            # db2 = dz2.sum(axis=0,keepdims=True)\n",
    "\n",
    "            # da1 = dz2 @ self.w[1].T # (batch,out(right)) @ (out(right),hidden(left)) = (batch,hidden)\n",
    "            # dz1 = da1 * self.relu_prime(z1) #dz = 0 if z<=0 else 1\n",
    "            # dw1 = (x.T @ dz1)\n",
    "            # db1 = dz1.sum(axis=0,keepdims=True)\n",
    "            \n",
    "\n",
    "            # self.w[0] -= lr*dw1\n",
    "            # self.b[0] -= lr*db1\n",
    "            # self.w[1] -= lr*dw2\n",
    "            # self.b[1] -= lr*db2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3) (64, 1) 1\n",
      "(64, 2) (64, 3) 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (64,2) (64,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 73\u001b[0m, in \u001b[0;36mMLP.train\u001b[1;34m(self, x, y, epoch, lr)\u001b[0m\n\u001b[0;32m     71\u001b[0m     da \u001b[38;5;241m=\u001b[39m dz \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw[i]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(da\u001b[38;5;241m.\u001b[39mshape,z[i]\u001b[38;5;241m.\u001b[39mshape,i)\n\u001b[1;32m---> 73\u001b[0m     dz \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_prime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m dw \u001b[38;5;241m=\u001b[39m (a[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m dz)\n\u001b[0;32m     75\u001b[0m db \u001b[38;5;241m=\u001b[39m dz\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (64,2) (64,3) "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = MLP([2,3,1])\n",
    "x = np.random.rand(64,2)\n",
    "y = np.random.rand(64,1)\n",
    "model.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37384904 0.93556763 0.72177065]\n",
      " [0.59797668 0.14108111 0.14591216]] [[ 0.70665659 -0.01043557  0.94897219]]\n",
      "[[0.37393544 0.91997898 0.71195717]\n",
      " [0.59806113 0.12578111 0.13628008]] [[ 0.70683177 -0.04218498  0.92898436]]\n",
      "[[0.37477155 0.90567136 0.70362157]\n",
      " [0.59887699 0.11181432 0.12814274]] [[ 0.70852453 -0.07116322  0.91210107]]\n",
      "[[0.37620339 0.89260767 0.69660276]\n",
      " [0.60026635 0.09913935 0.12133036]] [[ 0.71140889 -0.09747025  0.89795894]]\n",
      "[[0.37808528 0.88089085 0.69073661]\n",
      " [0.60208194 0.08831482 0.11566986]] [[ 0.71518381 -0.11923351  0.88619056]]\n",
      "[[0.38028622 0.87030132 0.6858639 ]\n",
      " [0.60419343 0.0786862  0.11099452]] [[ 0.71958521 -0.13857553  0.87644556]]\n",
      "[[0.38269093 0.86083939 0.68183967]\n",
      " [0.60648723 0.0703608  0.10715542]] [[ 0.72438331 -0.1552477   0.86841572]]\n",
      "[[0.38520277 0.85233609 0.67853114]\n",
      " [0.6088697  0.06301402 0.10401698]] [[ 0.72938906 -0.17012118  0.86182221]]\n",
      "[[0.38774252 0.84475951 0.67582162]\n",
      " [0.61126487 0.05652231 0.10146144]] [[ 0.73444945 -0.18319416  0.85642357]]\n",
      "[[0.39024881 0.83811183 0.67360861]\n",
      " [0.61361437 0.05122097 0.0993867 ]] [[ 0.73944719 -0.19417007  0.85201075]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = MLP([2,3,1])\n",
    "x = np.random.rand(64,2)\n",
    "y = np.random.rand(64,1)\n",
    "model.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ai_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
